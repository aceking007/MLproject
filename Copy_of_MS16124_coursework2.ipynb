{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MS16124_coursework2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aceking007/MLproject/blob/master/Copy_of_MS16124_coursework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gu7d_cfAyo",
        "colab_type": "text"
      },
      "source": [
        "# NEURAL NETWORK ASSIGNMENT\n",
        "## Name : Arpit Omprakash\n",
        "## Roll number: MS16124\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CFAsaOttbzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Problem 1\n",
        "def load_data(filepath):\n",
        "    df = pd.read_csv(filepath, header=None) # header=None assigns integer headers starting at 0\n",
        "    # df = df[0:20]\n",
        "    # print (df.head())\n",
        "    Y = df.iloc[:,784] # Just the last column\n",
        "    list_columns = list(range(df.shape[1])) # Makes a list of indices\n",
        "    list_columns.remove(784) # Removes the last column from list\n",
        "    X = df.iloc[:,list_columns] # All data except last column\n",
        "    X = X/255 # Normalization\n",
        "    # Arpit Omprakash\n",
        "    return X, Y\n",
        "\n",
        "X_train, Y_train = load_data(\"https://raw.githubusercontent.com/aceking007/MLproject/master/mnist_train.csv\")\n",
        "\n",
        "print(X_train.head())\n",
        "print(Y_train.head())\n",
        "print(X_train.shape, Y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG_3Tzs5gzXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 2 (No hidden layer)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip extreme values and thus prevents the loss from being infinite \n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    # Arpit Omprakash \n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Initialization\n",
        "def initialize_perceptron(X):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights = np.random.randn(input_neurons,output_neurons)\n",
        "    bias = np.random.randn(output_neurons)\n",
        "    return weights, bias\n",
        "\n",
        "def update_weights_perceptron(X,Y,weights,bias,lr):\n",
        "    # Feedforward\n",
        "    zoutput = np.dot(X,weights) + bias\n",
        "    predictions = softmax(zoutput)\n",
        "    # Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    # Printing the loss value\n",
        "    print(\"Loss = \",cross_entropy(predictions,target))\n",
        "    n = predictions.shape[0]\n",
        "    dcost_dzo = np.asarray(predictions-target)\n",
        "    dcost_weights_output = np.dot(X.T,dcost_dzo)\n",
        "    dcost_bias_output = np.asarray(np.sum(dcost_dzo,axis=0)/n)\n",
        "    # Arpit Omprakash\n",
        "    # Updating weights and bias\n",
        "    updated_weights = weights-lr*dcost_weights_output\n",
        "    updated_bias = bias-lr*dcost_bias_output\n",
        "    return updated_weights, updated_bias\n",
        "\n",
        "weights, bias = initialize_perceptron(X_train)\n",
        "for i in range(500):\n",
        "    print ('Epoch #',i)\n",
        "    weights,bias = update_weights_perceptron(X_train,Y_train,weights,bias,lr = 0.001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qObXUJoKf3ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 3 (Single hidden layer with N neurons)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip extreme values and thus prevents the loss from being infinite \n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_der(x):\n",
        "    return sigmoid(x) *(1-sigmoid (x))\n",
        "\n",
        "# Initialization\n",
        "def initialize_single_layer(X, hidden_layer_neurons):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights_hiddenlayer = np.random.randn(input_neurons,hidden_layer_neurons)\n",
        "    bias_hiddenlayer = np.random.randn(hidden_layer_neurons)\n",
        "    weights_outputlayer = np.random.randn(hidden_layer_neurons,output_neurons)\n",
        "    bias_outputlayer = np.random.randn(output_neurons)\n",
        "    weights = [weights_hiddenlayer,weights_outputlayer]\n",
        "    bias = [bias_hiddenlayer,bias_outputlayer]\n",
        "    return weights, bias    \n",
        "\n",
        "def update_weights_single_layer(X,Y,weights,bias,lr):\n",
        "    # Feedforward\n",
        "    # For the hidden layer\n",
        "    zhidden = np.dot(X,weights[0]) + bias[0]\n",
        "    ahidden = sigmoid(zhidden)\n",
        "    # For the output layer\n",
        "    zoutput = np.dot(ahidden,weights[1]) + bias[1]\n",
        "    aoutput = softmax(zoutput)\n",
        "    # Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    predictions = aoutput\n",
        "    # Printing the loss value\n",
        "    print(\"Loss = \",cross_entropy(predictions,target))\n",
        "    # For the output layer\n",
        "    dcost_dzo = predictions - target\n",
        "    dzo_dwo = ahidden\n",
        "    dcost_weights_output = np.dot(dzo_dwo.T,dcost_dzo)\n",
        "    dcost_bias_output = dcost_dzo\n",
        "    # For the hidden layer\n",
        "    # Arpit Omprakash\n",
        "    dzo_dah = weights[1]\n",
        "    dcost_dah = np.dot(dcost_dzo, dzo_dah.T)\n",
        "    dah_dzh = sigmoid_der(zhidden)\n",
        "    dcost_dzh = dah_dzh * dcost_dah\n",
        "    dzh_dwh = X\n",
        "    dcost_weights_hidden = np.dot(dzh_dwh.T, dcost_dzh)\n",
        "    dcost_bias_hidden = dcost_dzh\n",
        "    # Updating weights and biases\n",
        "    updated_weights = [weights[0]-lr*dcost_weights_hidden,weights[1]-lr*dcost_weights_output]\n",
        "    updated_bias = [bias[0]-lr*dcost_bias_hidden,bias[1]-lr*dcost_bias_output]\n",
        "    return updated_weights,updated_bias\n",
        "\n",
        "\n",
        "weights, bias = initialize_single_layer(X_train,hidden_layer_neurons = 12)\n",
        "for i in range(500):\n",
        "    print ('Epoch #',i)\n",
        "    weights,bias = update_weights_single_layer(X_train,Y_train,weights,bias,lr = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAHyRXd1g7l3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 4 (Two hidden layers with N neurons each)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip extreme values and thus prevents the loss from being infinite \n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_der(x):\n",
        "    return sigmoid(x) *(1-sigmoid (x))\n",
        "\n",
        "# Initialization\n",
        "def initialize_double_layer(X, hidden_layer_neurons):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights_h1 = np.random.randn(input_neurons,hidden_layer_neurons)\n",
        "    bias_h1 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_h2 = np.random.randn(hidden_layer_neurons,hidden_layer_neurons)\n",
        "    bias_h2 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_o = np.random.randn(hidden_layer_neurons,output_neurons)\n",
        "    bias_o = np.random.randn(output_neurons)\n",
        "    weights = [weights_h1, weights_h2,weights_o]\n",
        "    bias = [bias_h1, bias_h2, bias_o]\n",
        "    return weights, bias\n",
        "\n",
        "def update_weights_double_layer(X,Y,weights,bias,lr):\n",
        "    # Feedforward\n",
        "    # For h1 layer (hidden)\n",
        "    zh1 = np.dot(X,weights[0]) + bias[0]\n",
        "    ah1 = sigmoid(zh1)\n",
        "    # For h2 layer(hidden)\n",
        "    zh2 = np.dot(ah1,weights[1]) + bias[1]\n",
        "    ah2 = sigmoid(zh2)\n",
        "    # For output layer\n",
        "    zo = np.dot(ah2,weights[2]) + bias[2]\n",
        "    ao = softmax(zo)\n",
        "    # Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    predictions = ao\n",
        "    # Printing loss value\n",
        "    # Arpit Omprakash\n",
        "    print(\"Loss: \", cross_entropy(predictions,target))\n",
        "    # For the output layer\n",
        "    dcost_dzo = predictions - target\n",
        "    dzo_dwo = ah2\n",
        "    dcost_weights_o = np.dot(dzo_dwo.T,dcost_dzo)\n",
        "    dcost_bias_o = dcost_dzo\n",
        "    # For h2 layer (hidden)\n",
        "    dzo_dah2 = weights[2]\n",
        "    dcost_dah2 = np.dot(dcost_dzo , dzo_dah2.T)\n",
        "    dah2_dzh2 = sigmoid_der(zh2)\n",
        "    dcost_dzh2 = dah2_dzh2 * dcost_dah2\n",
        "    dzh2_dwh2 = ah1\n",
        "    dcost_weights_h2 = np.dot(dzh2_dwh2.T, dcost_dzh2)\n",
        "    dcost_bias_h2 = dcost_dzh2\n",
        "    # For h1 layer (hidden)\n",
        "    dzh2_dah1 = weights[1]\n",
        "    dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)\n",
        "    dah1_dzh1 = sigmoid_der(zh1)\n",
        "    dcost_dzh1 = dah1_dzh1 * dcost_dah1\n",
        "    dzh1_dwh1 = X\n",
        "    dcost_weights_h1 = np.dot(dzh1_dwh1.T, dcost_dzh1)\n",
        "    dcost_bias_h1 = dcost_dzh1\n",
        "    # Updating weights and biases\n",
        "    updated_weights = [weights[0]-lr*dcost_weights_h1,weights[1]-lr*dcost_weights_h2,weights[2]-lr*dcost_weights_o]\n",
        "    updated_bias = [bias[0]-lr*dcost_bias_h1,bias[1]-lr*dcost_bias_h2,bias[2]-lr*dcost_bias_o]\n",
        "    return updated_weights,updated_bias\n",
        "\n",
        "weights, bias = initialize_double_layer(X_train, hidden_layer_neurons = 10)\n",
        "for i in range(500):\n",
        "    print ('Epoch #',i)\n",
        "    weights,bias = update_weights_double_layer(X_train,Y_train,weights,bias,lr = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhaZIijiqI6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 5 (Two hidden layers with activation function)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip extreme values and thus prevents the loss from being infinite \n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_der(x):\n",
        "    return sigmoid(x) *(1-sigmoid (x))\n",
        "\n",
        "# TanH function\n",
        "def tanh(x):\n",
        "    temp = 2/(1+np.exp(-2*x))\n",
        "    return temp - 1\n",
        "\n",
        "# Derivative of TanH function\n",
        "def tanh_der(x):\n",
        "    return 1 - (tanh(x)*tanh(x))\n",
        "\n",
        "# ReLU function\n",
        "def relu(x):\n",
        "    return x * (x > 0) \n",
        "\n",
        "# Derivative of ReLU function\n",
        "def relu_der(x):\n",
        "    return 1. * (x > 0)\n",
        "\n",
        "# References to functions and derivatives\n",
        "f1 = sigmoid\n",
        "f2 = tanh\n",
        "f3 = relu\n",
        "f = [f1, f2, f3]\n",
        "d1 = sigmoid_der\n",
        "d2 = tanh_der\n",
        "d3 = relu_der\n",
        "d = [d1, d2, d3]\n",
        "\n",
        "# Exception class for any error in the parameters provided\n",
        "class ParameterError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Initialization\n",
        "def initialize_double_layer_act(X, hidden_layer_neurons):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights_h1 = np.random.randn(input_neurons,hidden_layer_neurons)\n",
        "    bias_h1 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_h2 = np.random.randn(hidden_layer_neurons,hidden_layer_neurons)\n",
        "    bias_h2 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_o = np.random.randn(hidden_layer_neurons,output_neurons)\n",
        "    bias_o = np.random.randn(output_neurons)\n",
        "    weights = [weights_h1, weights_h2,weights_o]\n",
        "    bias = [bias_h1, bias_h2, bias_o]\n",
        "    return weights, bias\n",
        "\n",
        "def update_weights_double_layer_act(X,Y,weights,bias,lr,activation):\n",
        "    # Changing activation function according to the parameter\n",
        "    if activation == 'sigmoid':\n",
        "        func = f1\n",
        "        deriv = d1\n",
        "    elif activation == 'tanh':\n",
        "        func = f2\n",
        "        deriv = d2\n",
        "    elif activation == 'relu':\n",
        "        func = f3\n",
        "        deriv = d3\n",
        "    else:\n",
        "        print(\"Invalid activation function\")\n",
        "        raise ParameterError('Activation function incorrect')\n",
        "    # Feedforward\n",
        "    # For h1 layer (hidden)\n",
        "    zh1 = np.dot(X,weights[0]) + bias[0]\n",
        "    ah1 = func(zh1)\n",
        "    # For h2 layer(hidden)\n",
        "    zh2 = np.dot(ah1,weights[1]) + bias[1]\n",
        "    ah2 = func(zh2)\n",
        "    # For output layer\n",
        "    zo = np.dot(ah2,weights[2]) + bias[2]\n",
        "    ao = softmax(zo)\n",
        "    # Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    predictions = ao\n",
        "    # Printing loss value\n",
        "    print(\"Loss: \", cross_entropy(predictions,target))\n",
        "    # For the output layer\n",
        "    # Arpit Omprakash\n",
        "    dcost_dzo = predictions - target\n",
        "    dzo_dwo = ah2\n",
        "    dcost_weights_o = np.dot(dzo_dwo.T,dcost_dzo)\n",
        "    dcost_bias_o = dcost_dzo\n",
        "    # For h2 layer (hidden)\n",
        "    dzo_dah2 = weights[2]\n",
        "    dcost_dah2 = np.dot(dcost_dzo , dzo_dah2.T)\n",
        "    dah2_dzh2 = deriv(zh2)\n",
        "    dcost_dzh2 = dah2_dzh2 * dcost_dah2\n",
        "    dzh2_dwh2 = ah1\n",
        "    dcost_weights_h2 = np.dot(dzh2_dwh2.T, dcost_dzh2)\n",
        "    dcost_bias_h2 = dcost_dzh2\n",
        "    # For h1 layer (hidden)\n",
        "    dzh2_dah1 = weights[1]\n",
        "    dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)\n",
        "    dah1_dzh1 = deriv(zh1)\n",
        "    dcost_dzh1 = dah1_dzh1 * dcost_dah1\n",
        "    dzh1_dwh1 = X\n",
        "    dcost_weights_h1 = np.dot(dzh1_dwh1.T, dcost_dzh1)\n",
        "    dcost_bias_h1 = dcost_dzh1\n",
        "    # Updating weights and biases\n",
        "    updated_weights = [weights[0]-lr*dcost_weights_h1,weights[1]-lr*dcost_weights_h2,weights[2]-lr*dcost_weights_o]\n",
        "    updated_bias = [bias[0]-lr*dcost_bias_h1,bias[1]-lr*dcost_bias_h2,bias[2]-lr*dcost_bias_o]\n",
        "    return updated_weights,updated_bias\n",
        "\n",
        "weights, bias = initialize_double_layer_act(X_train,hidden_layer_neurons = 10)\n",
        "for i in range(500):\n",
        "    print ('Epoch #',i)\n",
        "    try:\n",
        "      weights,bias = update_weights_double_layer_act(X_train,Y_train,weights,bias,lr = 0.001,activation = 'sigmoid')\n",
        "    except ParameterError as e:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y3E2Lty1JnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 6 (Two hidden layers with activation function and momentum)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip values make the np.log(predictions) infinitely large\n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_der(x):\n",
        "    return sigmoid(x) *(1-sigmoid (x))\n",
        "\n",
        "# TanH function\n",
        "def tanh(x):\n",
        "    temp = 2/(1+np.exp(-2*x))\n",
        "    return temp - 1\n",
        "\n",
        "# Derivative of TanH function\n",
        "def tanh_der(x):\n",
        "    return 1 - (tanh(x)*tanh(x))\n",
        "\n",
        "# ReLU function\n",
        "def relu(x):\n",
        "    return x * (x > 0) \n",
        "\n",
        "# Derivative of ReLU function\n",
        "def relu_der(x):\n",
        "    return 1. * (x > 0)\n",
        "\n",
        "# References to functions and derivatives\n",
        "f1 = sigmoid\n",
        "f2 = tanh\n",
        "f3 = relu\n",
        "f = [f1, f2, f3]\n",
        "d1 = sigmoid_der\n",
        "d2 = tanh_der\n",
        "d3 = relu_der\n",
        "d = [d1, d2, d3]\n",
        "\n",
        "# Exception class for any error in the parameters provided\n",
        "class ParameterError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Initialization\n",
        "def initialize_double_layer_act(X, hidden_layer_neurons):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights_h1 = np.random.randn(input_neurons,hidden_layer_neurons)\n",
        "    bias_h1 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_h2 = np.random.randn(hidden_layer_neurons,hidden_layer_neurons)\n",
        "    bias_h2 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_o = np.random.randn(hidden_layer_neurons,output_neurons)\n",
        "    bias_o = np.random.randn(output_neurons)\n",
        "    weights = [weights_h1, weights_h2,weights_o]\n",
        "    bias = [bias_h1, bias_h2, bias_o]\n",
        "    return weights, bias\n",
        "\n",
        "# Main implementation\n",
        "def update_weights_double_layer_act(X,Y,weights,bias,lr,activation,momentum):\n",
        "    global prev_dcost_weights_o, prev_dcost_bias_o, prev_dcost_weights_h2, prev_dcost_bias_h2, prev_dcost_weights_h1, prev_dcost_bias_h1\n",
        "    # Changing activation function according to the parameter\n",
        "    if activation == 'sigmoid':\n",
        "        func = f1\n",
        "        deriv = d1\n",
        "    elif activation == 'tanh':\n",
        "        func = f2\n",
        "        deriv = d2\n",
        "    elif activation == 'relu':\n",
        "        func = f3\n",
        "        deriv = d3\n",
        "    else:\n",
        "        print(\"Invalid activation function\")\n",
        "        raise ParameterError('Activation function incorrect')\n",
        "    ###### Feedforward\n",
        "    ### For h1 layer (hidden)\n",
        "    zh1 = np.dot(X,weights[0]) + bias[0]\n",
        "    ah1 = func(zh1)\n",
        "    ### For h2 layer(hidden)\n",
        "    zh2 = np.dot(ah1,weights[1]) + bias[1]\n",
        "    ah2 = func(zh2)\n",
        "    ### For output layer\n",
        "    zo = np.dot(ah2,weights[2]) + bias[2]\n",
        "    ao = softmax(zo)\n",
        "    ###### Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    predictions = ao\n",
        "    # Printing loss value\n",
        "    print(\"Loss: \", cross_entropy(predictions,target))\n",
        "    ### For the output layer\n",
        "    dcost_dzo = predictions - target\n",
        "    dzo_dwo = ah2\n",
        "    # Updating weights in accordance with previous weights using momentum\n",
        "    dcost_weights_o = momentum*prev_dcost_weights_o + (1-momentum)*np.dot(dzo_dwo.T,dcost_dzo)\n",
        "    dcost_bias_o = momentum*prev_dcost_bias_o + (1-momentum)*dcost_dzo\n",
        "    # Changing previous weights to current weights\n",
        "    prev_dcost_weights_o = dcost_weights_o\n",
        "    prev_dcost_bias_o = dcost_bias_o\n",
        "    ### For h2 layer (hidden)\n",
        "    dzo_dah2 = weights[2]\n",
        "    dcost_dah2 = np.dot(dcost_dzo , dzo_dah2.T)\n",
        "    dah2_dzh2 = deriv(zh2)\n",
        "    dcost_dzh2 = dah2_dzh2 * dcost_dah2\n",
        "    dzh2_dwh2 = ah1\n",
        "    # Updating weights in accordance with previous weights using momentum\n",
        "    dcost_weights_h2 = momentum*prev_dcost_weights_h2 + (1-momentum)*np.dot(dzh2_dwh2.T, dcost_dzh2)\n",
        "    dcost_bias_h2 = momentum*prev_dcost_bias_h2 + (1-momentum)*dcost_dzh2\n",
        "    # Changing previous weights to current weights\n",
        "    prev_dcost_weights_h2 = dcost_weights_h2\n",
        "    prev_dcost_bias_h2 = dcost_bias_h2\n",
        "    ### For h1 layer (hidden)\n",
        "    dzh2_dah1 = weights[1]\n",
        "    dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)\n",
        "    dah1_dzh1 = deriv(zh1)\n",
        "    dcost_dzh1 = dah1_dzh1 * dcost_dah1\n",
        "    dzh1_dwh1 = X\n",
        "    # Updating weights in accordance with previous weights using momentum\n",
        "    # Arpit Omprakash\n",
        "    dcost_weights_h1 = momentum*prev_dcost_weights_h1 + (1-momentum)*np.dot(dzh1_dwh1.T, dcost_dzh1)\n",
        "    dcost_bias_h1 = momentum*prev_dcost_bias_h1 + (1-momentum)*dcost_dzh1\n",
        "    # Changing previous weights to current weights\n",
        "    prev_dcost_weights_h1 = dcost_weights_h1\n",
        "    prev_dcost_bias_h1 = dcost_bias_h1\n",
        "    ###### Updating weights and biases\n",
        "    updated_weights = [weights[0]-lr*dcost_weights_h1,weights[1]-lr*dcost_weights_h2,weights[2]-lr*dcost_weights_o]\n",
        "    updated_bias = [bias[0]-lr*dcost_bias_h1,bias[1]-lr*dcost_bias_h2,bias[2]-lr*dcost_bias_o]\n",
        "    return updated_weights,updated_bias\n",
        "\n",
        "# Function that can takes epoch value and trains the neural network\n",
        "def training_nn(X,Y,weights,bias,lr,activation,momentum,epochs = 10):\n",
        "    for epoch in range(epochs):\n",
        "        print ('Epoch #', epoch)\n",
        "        try:\n",
        "          weights,bias = update_weights_double_layer_act(X,Y,weights,bias,lr,activation,momentum)\n",
        "        except ParameterError:\n",
        "          break\n",
        "    return weights\n",
        "\n",
        "# Variables to store previous gradients (for implementing momentum)\n",
        "prev_dcost_weights_o = 0\n",
        "prev_dcost_bias_o = 0\n",
        "prev_dcost_weights_h2 = 0\n",
        "prev_dcost_bias_h2 = 0\n",
        "prev_dcost_weights_h1 = 0\n",
        "prev_dcost_bias_h1 = 0\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Initializing weights and bias\n",
        "weights, bias = initialize_double_layer_act(X_train,hidden_layer_neurons = 10)\n",
        "# Training the model for \"epoch\" number of epochs\n",
        "# The training returns the resulting weights after training\n",
        "training_nn(X_train, Y_train, weights, bias, lr = 0.01, activation = 'sigmoid', momentum = 0.9, epochs = 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}