{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MS16124_coursework2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aceking007/MLproject/blob/master/Copy_of_MS16124_coursework2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7gu7d_cfAyo",
        "colab_type": "text"
      },
      "source": [
        "# NEURAL NETWORK ASSIGNMENT\n",
        "## Name : Arpit Omprakash\n",
        "## Roll number: MS16124\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CFAsaOttbzH",
        "colab_type": "code",
        "outputId": "8823529b-53d8-4921-b7e8-f15a9b134997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Problem 1\n",
        "def load_data(filepath):\n",
        "    df = pd.read_csv(filepath, header=None) # header=None assigns integer headers starting at 0\n",
        "    # df = df[0:20]\n",
        "    # print (df.head())\n",
        "    Y = df.iloc[:,784] # Just the last column\n",
        "    list_columns = list(range(df.shape[1])) # Makes a list of indices\n",
        "    list_columns.remove(784) # Removes the last column from list\n",
        "    X = df.iloc[:,list_columns] # All data except last column\n",
        "    X = X/255 # Normalization\n",
        "    # Arpit Omprakash\n",
        "    return X, Y\n",
        "\n",
        "X_train, Y_train = load_data(\"https://raw.githubusercontent.com/aceking007/MLproject/master/mnist_train.csv\")\n",
        "\n",
        "print(X_train.head())\n",
        "print(Y_train.head())\n",
        "print(X_train.shape, Y_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0    1    2    3    4    5    6    ...  777  778  779  780  781  782  783\n",
            "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
            "\n",
            "[5 rows x 784 columns]\n",
            "0    5\n",
            "1    0\n",
            "2    4\n",
            "3    1\n",
            "4    9\n",
            "Name: 784, dtype: int64\n",
            "(10000, 784) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG_3Tzs5gzXp",
        "colab_type": "code",
        "outputId": "9a6a72b9-7f1a-485c-a89e-2697726a0a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Problem 2 (No hidden layer)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip extreme values and thus prevents the loss from being infinite \n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    # Arpit Omprakash \n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Initialization\n",
        "def initialize_perceptron(X):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights = np.random.randn(input_neurons,output_neurons)\n",
        "    bias = np.random.randn(output_neurons)\n",
        "    return weights, bias\n",
        "\n",
        "def update_weights_perceptron(X,Y,weights,bias,lr):\n",
        "    # Feedforward\n",
        "    zoutput = np.dot(X,weights) + bias\n",
        "    predictions = softmax(zoutput)\n",
        "    # Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    # Printing the loss value\n",
        "    print(\"Loss = \",cross_entropy(predictions,target))\n",
        "    n = predictions.shape[0]\n",
        "    dcost_dzo = np.asarray(predictions-target)\n",
        "    dcost_weights_output = np.dot(X.T,dcost_dzo)\n",
        "    dcost_bias_output = np.asarray(np.sum(dcost_dzo,axis=0)/n)\n",
        "    # Arpit Omprakash\n",
        "    # Updating weights and bias\n",
        "    updated_weights = weights-lr*dcost_weights_output\n",
        "    updated_bias = bias-lr*dcost_bias_output\n",
        "    return updated_weights, updated_bias\n",
        "\n",
        "weights, bias = initialize_perceptron(X_train)\n",
        "for i in range(500):\n",
        "    print ('Epoch #',i)\n",
        "    weights,bias = update_weights_perceptron(X_train,Y_train,weights,bias,lr = 0.001)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch # 0\n",
            "Loss =  13.276942885909849\n",
            "Epoch # 1\n",
            "Loss =  9.630783034607978\n",
            "Epoch # 2\n",
            "Loss =  12.729613142667345\n",
            "Epoch # 3\n",
            "Loss =  11.753724548814159\n",
            "Epoch # 4\n",
            "Loss =  10.236582188821233\n",
            "Epoch # 5\n",
            "Loss =  9.449523975525818\n",
            "Epoch # 6\n",
            "Loss =  9.510597842697063\n",
            "Epoch # 7\n",
            "Loss =  4.436928934467116\n",
            "Epoch # 8\n",
            "Loss =  4.601636484382475\n",
            "Epoch # 9\n",
            "Loss =  7.694069078026279\n",
            "Epoch # 10\n",
            "Loss =  5.718336791687288\n",
            "Epoch # 11\n",
            "Loss =  3.848504897310665\n",
            "Epoch # 12\n",
            "Loss =  2.3108993783821643\n",
            "Epoch # 13\n",
            "Loss =  1.784934369454622\n",
            "Epoch # 14\n",
            "Loss =  1.900064743771425\n",
            "Epoch # 15\n",
            "Loss =  2.3414260796423494\n",
            "Epoch # 16\n",
            "Loss =  3.191555947190077\n",
            "Epoch # 17\n",
            "Loss =  3.0508042907175295\n",
            "Epoch # 18\n",
            "Loss =  1.6794253999886284\n",
            "Epoch # 19\n",
            "Loss =  1.4417712152776767\n",
            "Epoch # 20\n",
            "Loss =  1.513401870131187\n",
            "Epoch # 21\n",
            "Loss =  1.7488078347332456\n",
            "Epoch # 22\n",
            "Loss =  2.201083620306814\n",
            "Epoch # 23\n",
            "Loss =  2.851029887960488\n",
            "Epoch # 24\n",
            "Loss =  2.7248193853540092\n",
            "Epoch # 25\n",
            "Loss =  1.6758214919264025\n",
            "Epoch # 26\n",
            "Loss =  1.5261324982824849\n",
            "Epoch # 27\n",
            "Loss =  1.6263840283639166\n",
            "Epoch # 28\n",
            "Loss =  1.4905497973772\n",
            "Epoch # 29\n",
            "Loss =  1.6420174545498445\n",
            "Epoch # 30\n",
            "Loss =  1.4616695112323297\n",
            "Epoch # 31\n",
            "Loss =  1.603350239374402\n",
            "Epoch # 32\n",
            "Loss =  1.4252369644602298\n",
            "Epoch # 33\n",
            "Loss =  1.5270718579535252\n",
            "Epoch # 34\n",
            "Loss =  1.3431243525672498\n",
            "Epoch # 35\n",
            "Loss =  1.4009310572574964\n",
            "Epoch # 36\n",
            "Loss =  1.2383539729500592\n",
            "Epoch # 37\n",
            "Loss =  1.273081218695572\n",
            "Epoch # 38\n",
            "Loss =  1.1673929145653321\n",
            "Epoch # 39\n",
            "Loss =  1.189310480123234\n",
            "Epoch # 40\n",
            "Loss =  1.1532529138875538\n",
            "Epoch # 41\n",
            "Loss =  1.1806155739035349\n",
            "Epoch # 42\n",
            "Loss =  1.2531308258308569\n",
            "Epoch # 43\n",
            "Loss =  1.3427442329253891\n",
            "Epoch # 44\n",
            "Loss =  1.7264956448682853\n",
            "Epoch # 45\n",
            "Loss =  2.403781143150366\n",
            "Epoch # 46\n",
            "Loss =  3.8729323586572812\n",
            "Epoch # 47\n",
            "Loss =  3.306718290331648\n",
            "Epoch # 48\n",
            "Loss =  1.2180586768994797\n",
            "Epoch # 49\n",
            "Loss =  0.9898121975380347\n",
            "Epoch # 50\n",
            "Loss =  0.9414698741543354\n",
            "Epoch # 51\n",
            "Loss =  0.9205234117205684\n",
            "Epoch # 52\n",
            "Loss =  0.896101649377766\n",
            "Epoch # 53\n",
            "Loss =  0.8862250920691902\n",
            "Epoch # 54\n",
            "Loss =  0.8658321388935508\n",
            "Epoch # 55\n",
            "Loss =  0.8603886210508604\n",
            "Epoch # 56\n",
            "Loss =  0.8455167701408677\n",
            "Epoch # 57\n",
            "Loss =  0.8474540691968845\n",
            "Epoch # 58\n",
            "Loss =  0.8379223299779096\n",
            "Epoch # 59\n",
            "Loss =  0.8522415615999674\n",
            "Epoch # 60\n",
            "Loss =  0.8514745457483439\n",
            "Epoch # 61\n",
            "Loss =  0.8886897748554374\n",
            "Epoch # 62\n",
            "Loss =  0.9214494487407373\n",
            "Epoch # 63\n",
            "Loss =  1.0368895638655296\n",
            "Epoch # 64\n",
            "Loss =  1.2060963678690066\n",
            "Epoch # 65\n",
            "Loss =  1.8432377020505561\n",
            "Epoch # 66\n",
            "Loss =  2.747227823233385\n",
            "Epoch # 67\n",
            "Loss =  2.45784343743274\n",
            "Epoch # 68\n",
            "Loss =  2.739816006083719\n",
            "Epoch # 69\n",
            "Loss =  4.538384831187656\n",
            "Epoch # 70\n",
            "Loss =  4.886196970536847\n",
            "Epoch # 71\n",
            "Loss =  4.350925127262161\n",
            "Epoch # 72\n",
            "Loss =  1.8996105261698042\n",
            "Epoch # 73\n",
            "Loss =  1.527973858372264\n",
            "Epoch # 74\n",
            "Loss =  1.8967750932607177\n",
            "Epoch # 75\n",
            "Loss =  1.3869833259329687\n",
            "Epoch # 76\n",
            "Loss =  1.298512223038638\n",
            "Epoch # 77\n",
            "Loss =  0.9758676888837093\n",
            "Epoch # 78\n",
            "Loss =  0.8833945890856821\n",
            "Epoch # 79\n",
            "Loss =  0.8250153082132141\n",
            "Epoch # 80\n",
            "Loss =  0.8037110682083786\n",
            "Epoch # 81\n",
            "Loss =  0.786810351326419\n",
            "Epoch # 82\n",
            "Loss =  0.7738445728681409\n",
            "Epoch # 83\n",
            "Loss =  0.7603853624089459\n",
            "Epoch # 84\n",
            "Loss =  0.7493392300797388\n",
            "Epoch # 85\n",
            "Loss =  0.7374635560657689\n",
            "Epoch # 86\n",
            "Loss =  0.7276027122705592\n",
            "Epoch # 87\n",
            "Loss =  0.7168423574527777\n",
            "Epoch # 88\n",
            "Loss =  0.7080291987129607\n",
            "Epoch # 89\n",
            "Loss =  0.6981170865221695\n",
            "Epoch # 90\n",
            "Loss =  0.6902617035442565\n",
            "Epoch # 91\n",
            "Loss =  0.6810199934045964\n",
            "Epoch # 92\n",
            "Loss =  0.6741067611337582\n",
            "Epoch # 93\n",
            "Loss =  0.6654561472297876\n",
            "Epoch # 94\n",
            "Loss =  0.6596382854449343\n",
            "Epoch # 95\n",
            "Loss =  0.6516722697310677\n",
            "Epoch # 96\n",
            "Loss =  0.6475730961717598\n",
            "Epoch # 97\n",
            "Loss =  0.6409272763340114\n",
            "Epoch # 98\n",
            "Loss =  0.6406780546009968\n",
            "Epoch # 99\n",
            "Loss =  0.6382989299878323\n",
            "Epoch # 100\n",
            "Loss =  0.6499204371063033\n",
            "Epoch # 101\n",
            "Loss =  0.6647573421288572\n",
            "Epoch # 102\n",
            "Loss =  0.7199395653373777\n",
            "Epoch # 103\n",
            "Loss =  0.8070076684725144\n",
            "Epoch # 104\n",
            "Loss =  1.168868913632017\n",
            "Epoch # 105\n",
            "Loss =  2.417977446874408\n",
            "Epoch # 106\n",
            "Loss =  3.7396661696115463\n",
            "Epoch # 107\n",
            "Loss =  4.095355147538626\n",
            "Epoch # 108\n",
            "Loss =  3.7933928915281516\n",
            "Epoch # 109\n",
            "Loss =  1.9806095172061224\n",
            "Epoch # 110\n",
            "Loss =  1.8771430494501318\n",
            "Epoch # 111\n",
            "Loss =  2.54002796286051\n",
            "Epoch # 112\n",
            "Loss =  2.7008970862247077\n",
            "Epoch # 113\n",
            "Loss =  1.3422763635791182\n",
            "Epoch # 114\n",
            "Loss =  0.9360545749260601\n",
            "Epoch # 115\n",
            "Loss =  0.8122188513220172\n",
            "Epoch # 116\n",
            "Loss =  0.7709993227090712\n",
            "Epoch # 117\n",
            "Loss =  0.731696036725976\n",
            "Epoch # 118\n",
            "Loss =  0.7210855706487913\n",
            "Epoch # 119\n",
            "Loss =  0.6985747243090126\n",
            "Epoch # 120\n",
            "Loss =  0.6949679986118991\n",
            "Epoch # 121\n",
            "Loss =  0.6809525038355636\n",
            "Epoch # 122\n",
            "Loss =  0.681482812659144\n",
            "Epoch # 123\n",
            "Loss =  0.6736102069355377\n",
            "Epoch # 124\n",
            "Loss =  0.6798444329904494\n",
            "Epoch # 125\n",
            "Loss =  0.6812068058707126\n",
            "Epoch # 126\n",
            "Loss =  0.6986949528499609\n",
            "Epoch # 127\n",
            "Loss =  0.7206180205094233\n",
            "Epoch # 128\n",
            "Loss =  0.773547528102769\n",
            "Epoch # 129\n",
            "Loss =  0.8690853388083682\n",
            "Epoch # 130\n",
            "Loss =  1.091636022242256\n",
            "Epoch # 131\n",
            "Loss =  1.8019415964435614\n",
            "Epoch # 132\n",
            "Loss =  2.601831652311689\n",
            "Epoch # 133\n",
            "Loss =  1.6392191909621132\n",
            "Epoch # 134\n",
            "Loss =  1.5404731203896098\n",
            "Epoch # 135\n",
            "Loss =  2.2906221490711025\n",
            "Epoch # 136\n",
            "Loss =  2.8919281579519316\n",
            "Epoch # 137\n",
            "Loss =  1.4224424540283236\n",
            "Epoch # 138\n",
            "Loss =  1.186712770568109\n",
            "Epoch # 139\n",
            "Loss =  1.595509266501999\n",
            "Epoch # 140\n",
            "Loss =  2.300760463495377\n",
            "Epoch # 141\n",
            "Loss =  2.7070775891781444\n",
            "Epoch # 142\n",
            "Loss =  0.9039493564396185\n",
            "Epoch # 143\n",
            "Loss =  0.7496075062811608\n",
            "Epoch # 144\n",
            "Loss =  0.739666232758866\n",
            "Epoch # 145\n",
            "Loss =  0.7111152298160255\n",
            "Epoch # 146\n",
            "Loss =  0.7167999114440313\n",
            "Epoch # 147\n",
            "Loss =  0.6935234770791513\n",
            "Epoch # 148\n",
            "Loss =  0.7010577634838181\n",
            "Epoch # 149\n",
            "Loss =  0.6801312536936918\n",
            "Epoch # 150\n",
            "Loss =  0.6933999587239198\n",
            "Epoch # 151\n",
            "Loss =  0.6757517515923847\n",
            "Epoch # 152\n",
            "Loss =  0.6997128396675237\n",
            "Epoch # 153\n",
            "Loss =  0.687797336108988\n",
            "Epoch # 154\n",
            "Loss =  0.7390019574457356\n",
            "Epoch # 155\n",
            "Loss =  0.7581316670844932\n",
            "Epoch # 156\n",
            "Loss =  0.9570885719284142\n",
            "Epoch # 157\n",
            "Loss =  1.3058924563392407\n",
            "Epoch # 158\n",
            "Loss =  2.364506633289981\n",
            "Epoch # 159\n",
            "Loss =  1.3742756749154537\n",
            "Epoch # 160\n",
            "Loss =  1.542812465364468\n",
            "Epoch # 161\n",
            "Loss =  1.7579280882145845\n",
            "Epoch # 162\n",
            "Loss =  2.5682056353674247\n",
            "Epoch # 163\n",
            "Loss =  1.3900035737479797\n",
            "Epoch # 164\n",
            "Loss =  1.660443654726569\n",
            "Epoch # 165\n",
            "Loss =  2.5133222192506275\n",
            "Epoch # 166\n",
            "Loss =  1.4537641387124456\n",
            "Epoch # 167\n",
            "Loss =  1.7167446233866177\n",
            "Epoch # 168\n",
            "Loss =  1.701273175497017\n",
            "Epoch # 169\n",
            "Loss =  2.103319933572812\n",
            "Epoch # 170\n",
            "Loss =  1.3648949293793873\n",
            "Epoch # 171\n",
            "Loss =  1.22913167630874\n",
            "Epoch # 172\n",
            "Loss =  1.027563048130157\n",
            "Epoch # 173\n",
            "Loss =  0.8171998628375768\n",
            "Epoch # 174\n",
            "Loss =  0.6994910023269616\n",
            "Epoch # 175\n",
            "Loss =  0.6459744333595009\n",
            "Epoch # 176\n",
            "Loss =  0.6300489486998485\n",
            "Epoch # 177\n",
            "Loss =  0.6122395151435687\n",
            "Epoch # 178\n",
            "Loss =  0.6020159308930134\n",
            "Epoch # 179\n",
            "Loss =  0.5903580827070518\n",
            "Epoch # 180\n",
            "Loss =  0.5831972172913292\n",
            "Epoch # 181\n",
            "Loss =  0.574143982095844\n",
            "Epoch # 182\n",
            "Loss =  0.5693462397403087\n",
            "Epoch # 183\n",
            "Loss =  0.5621145575888628\n",
            "Epoch # 184\n",
            "Loss =  0.5598591332806984\n",
            "Epoch # 185\n",
            "Loss =  0.5547334740206094\n",
            "Epoch # 186\n",
            "Loss =  0.5564018878627033\n",
            "Epoch # 187\n",
            "Loss =  0.5548111552068472\n",
            "Epoch # 188\n",
            "Loss =  0.5641891730575157\n",
            "Epoch # 189\n",
            "Loss =  0.5689328243868707\n",
            "Epoch # 190\n",
            "Loss =  0.5956901001716308\n",
            "Epoch # 191\n",
            "Loss =  0.6201383188707832\n",
            "Epoch # 192\n",
            "Loss =  0.7234961054563059\n",
            "Epoch # 193\n",
            "Loss =  0.9219992968438999\n",
            "Epoch # 194\n",
            "Loss =  1.7729273162105215\n",
            "Epoch # 195\n",
            "Loss =  2.744134457574265\n",
            "Epoch # 196\n",
            "Loss =  2.547483408875436\n",
            "Epoch # 197\n",
            "Loss =  2.1043032437470415\n",
            "Epoch # 198\n",
            "Loss =  1.119959207119348\n",
            "Epoch # 199\n",
            "Loss =  1.1733045124744996\n",
            "Epoch # 200\n",
            "Loss =  1.1265354858224987\n",
            "Epoch # 201\n",
            "Loss =  1.0582574450891657\n",
            "Epoch # 202\n",
            "Loss =  1.2323148374334207\n",
            "Epoch # 203\n",
            "Loss =  1.2085682348266995\n",
            "Epoch # 204\n",
            "Loss =  1.4699799260696405\n",
            "Epoch # 205\n",
            "Loss =  1.6518614670573446\n",
            "Epoch # 206\n",
            "Loss =  2.0346213102877027\n",
            "Epoch # 207\n",
            "Loss =  2.7113133776458604\n",
            "Epoch # 208\n",
            "Loss =  1.8731972272614876\n",
            "Epoch # 209\n",
            "Loss =  2.137473757622426\n",
            "Epoch # 210\n",
            "Loss =  1.8638553259576838\n",
            "Epoch # 211\n",
            "Loss =  2.409314045025355\n",
            "Epoch # 212\n",
            "Loss =  2.664679455707519\n",
            "Epoch # 213\n",
            "Loss =  1.705679452359393\n",
            "Epoch # 214\n",
            "Loss =  1.5185541393118582\n",
            "Epoch # 215\n",
            "Loss =  1.2079753455508981\n",
            "Epoch # 216\n",
            "Loss =  1.1136489113534145\n",
            "Epoch # 217\n",
            "Loss =  0.8770000193160186\n",
            "Epoch # 218\n",
            "Loss =  0.7570149263978905\n",
            "Epoch # 219\n",
            "Loss =  0.6632952828943106\n",
            "Epoch # 220\n",
            "Loss =  0.6223768644706329\n",
            "Epoch # 221\n",
            "Loss =  0.6043877132450436\n",
            "Epoch # 222\n",
            "Loss =  0.591220999803519\n",
            "Epoch # 223\n",
            "Loss =  0.579966670116792\n",
            "Epoch # 224\n",
            "Loss =  0.5706681997479514\n",
            "Epoch # 225\n",
            "Loss =  0.5615304281821993\n",
            "Epoch # 226\n",
            "Loss =  0.5536906657006523\n",
            "Epoch # 227\n",
            "Loss =  0.5457582948499419\n",
            "Epoch # 228\n",
            "Loss =  0.5388740221669367\n",
            "Epoch # 229\n",
            "Loss =  0.5317948950099219\n",
            "Epoch # 230\n",
            "Loss =  0.5256325796464429\n",
            "Epoch # 231\n",
            "Loss =  0.5192348259464811\n",
            "Epoch # 232\n",
            "Loss =  0.5136504030218393\n",
            "Epoch # 233\n",
            "Loss =  0.5078134313358134\n",
            "Epoch # 234\n",
            "Loss =  0.5027210071601045\n",
            "Epoch # 235\n",
            "Loss =  0.4973620295691382\n",
            "Epoch # 236\n",
            "Loss =  0.49272895062530325\n",
            "Epoch # 237\n",
            "Loss =  0.4878045156990065\n",
            "Epoch # 238\n",
            "Loss =  0.4836423357040292\n",
            "Epoch # 239\n",
            "Loss =  0.47915511160115054\n",
            "Epoch # 240\n",
            "Loss =  0.47554409379427054\n",
            "Epoch # 241\n",
            "Loss =  0.4715944488862261\n",
            "Epoch # 242\n",
            "Loss =  0.468763910029813\n",
            "Epoch # 243\n",
            "Loss =  0.4657182907098904\n",
            "Epoch # 244\n",
            "Loss =  0.46430016828250237\n",
            "Epoch # 245\n",
            "Loss =  0.4633857843728308\n",
            "Epoch # 246\n",
            "Loss =  0.4654360758648161\n",
            "Epoch # 247\n",
            "Loss =  0.47080562754203364\n",
            "Epoch # 248\n",
            "Loss =  0.4831184467049144\n",
            "Epoch # 249\n",
            "Loss =  0.5045332667053601\n",
            "Epoch # 250\n",
            "Loss =  0.5464757341771982\n",
            "Epoch # 251\n",
            "Loss =  0.6173124477678185\n",
            "Epoch # 252\n",
            "Loss =  0.8247582002311353\n",
            "Epoch # 253\n",
            "Loss =  1.524399862776266\n",
            "Epoch # 254\n",
            "Loss =  2.4990559998584216\n",
            "Epoch # 255\n",
            "Loss =  1.6629826922582343\n",
            "Epoch # 256\n",
            "Loss =  1.3904313119523186\n",
            "Epoch # 257\n",
            "Loss =  2.2993471036237647\n",
            "Epoch # 258\n",
            "Loss =  3.533262658076478\n",
            "Epoch # 259\n",
            "Loss =  3.5952812403290286\n",
            "Epoch # 260\n",
            "Loss =  2.07325924181691\n",
            "Epoch # 261\n",
            "Loss =  2.780996261187776\n",
            "Epoch # 262\n",
            "Loss =  2.3911219229122116\n",
            "Epoch # 263\n",
            "Loss =  2.311192269892683\n",
            "Epoch # 264\n",
            "Loss =  2.867773063294163\n",
            "Epoch # 265\n",
            "Loss =  1.666311834781447\n",
            "Epoch # 266\n",
            "Loss =  1.70202209921123\n",
            "Epoch # 267\n",
            "Loss =  1.7672632055142363\n",
            "Epoch # 268\n",
            "Loss =  1.370844975651032\n",
            "Epoch # 269\n",
            "Loss =  1.061255087723705\n",
            "Epoch # 270\n",
            "Loss =  0.7671417798904554\n",
            "Epoch # 271\n",
            "Loss =  0.6141606797832898\n",
            "Epoch # 272\n",
            "Loss =  0.5829479242400348\n",
            "Epoch # 273\n",
            "Loss =  0.5667896716282743\n",
            "Epoch # 274\n",
            "Loss =  0.5561301130756153\n",
            "Epoch # 275\n",
            "Loss =  0.5456610386407235\n",
            "Epoch # 276\n",
            "Loss =  0.5368000334753833\n",
            "Epoch # 277\n",
            "Loss =  0.5281223999170394\n",
            "Epoch # 278\n",
            "Loss =  0.5205076294652199\n",
            "Epoch # 279\n",
            "Loss =  0.5130447034138502\n",
            "Epoch # 280\n",
            "Loss =  0.506383186163903\n",
            "Epoch # 281\n",
            "Loss =  0.4998267995191246\n",
            "Epoch # 282\n",
            "Loss =  0.4939117827981825\n",
            "Epoch # 283\n",
            "Loss =  0.4880456595191651\n",
            "Epoch # 284\n",
            "Loss =  0.4827200130947038\n",
            "Epoch # 285\n",
            "Loss =  0.4773892707850353\n",
            "Epoch # 286\n",
            "Loss =  0.4725574685147186\n",
            "Epoch # 287\n",
            "Loss =  0.4676671720641593\n",
            "Epoch # 288\n",
            "Loss =  0.46327616574164987\n",
            "Epoch # 289\n",
            "Loss =  0.4587589614595385\n",
            "Epoch # 290\n",
            "Loss =  0.4547833500658412\n",
            "Epoch # 291\n",
            "Loss =  0.45060526609347734\n",
            "Epoch # 292\n",
            "Loss =  0.44708113043513276\n",
            "Epoch # 293\n",
            "Loss =  0.4432859327112168\n",
            "Epoch # 294\n",
            "Loss =  0.4404037808719503\n",
            "Epoch # 295\n",
            "Loss =  0.4372726820168662\n",
            "Epoch # 296\n",
            "Loss =  0.4357000267201351\n",
            "Epoch # 297\n",
            "Loss =  0.4343649406189744\n",
            "Epoch # 298\n",
            "Loss =  0.4364548210395853\n",
            "Epoch # 299\n",
            "Loss =  0.44115747187579546\n",
            "Epoch # 300\n",
            "Loss =  0.45618069858334365\n",
            "Epoch # 301\n",
            "Loss =  0.4801840824264802\n",
            "Epoch # 302\n",
            "Loss =  0.5491015877612748\n",
            "Epoch # 303\n",
            "Loss =  0.6419325272008546\n",
            "Epoch # 304\n",
            "Loss =  1.1173908200658984\n",
            "Epoch # 305\n",
            "Loss =  2.418975672130624\n",
            "Epoch # 306\n",
            "Loss =  2.4629211303808063\n",
            "Epoch # 307\n",
            "Loss =  1.9627798028420242\n",
            "Epoch # 308\n",
            "Loss =  1.1304199294770296\n",
            "Epoch # 309\n",
            "Loss =  1.428608044730633\n",
            "Epoch # 310\n",
            "Loss =  1.3393106657467706\n",
            "Epoch # 311\n",
            "Loss =  1.4660886672341062\n",
            "Epoch # 312\n",
            "Loss =  1.6222966767743885\n",
            "Epoch # 313\n",
            "Loss =  1.416760299532488\n",
            "Epoch # 314\n",
            "Loss =  1.6041537479208716\n",
            "Epoch # 315\n",
            "Loss =  1.2347766677972265\n",
            "Epoch # 316\n",
            "Loss =  0.9065486372084371\n",
            "Epoch # 317\n",
            "Loss =  0.6731649568415072\n",
            "Epoch # 318\n",
            "Loss =  0.5488291488040602\n",
            "Epoch # 319\n",
            "Loss =  0.5159902844756663\n",
            "Epoch # 320\n",
            "Loss =  0.50068344835285\n",
            "Epoch # 321\n",
            "Loss =  0.4845514381723905\n",
            "Epoch # 322\n",
            "Loss =  0.47729535636112913\n",
            "Epoch # 323\n",
            "Loss =  0.46681868225724277\n",
            "Epoch # 324\n",
            "Loss =  0.4619744095605373\n",
            "Epoch # 325\n",
            "Loss =  0.4544096706342712\n",
            "Epoch # 326\n",
            "Loss =  0.45110889749887356\n",
            "Epoch # 327\n",
            "Loss =  0.4457717522449943\n",
            "Epoch # 328\n",
            "Loss =  0.4440648978513038\n",
            "Epoch # 329\n",
            "Loss =  0.44107011101035687\n",
            "Epoch # 330\n",
            "Loss =  0.4415585481829435\n",
            "Epoch # 331\n",
            "Loss =  0.4418265053802721\n",
            "Epoch # 332\n",
            "Loss =  0.445646868780736\n",
            "Epoch # 333\n",
            "Loss =  0.4514154576003273\n",
            "Epoch # 334\n",
            "Loss =  0.46092164775319316\n",
            "Epoch # 335\n",
            "Loss =  0.4766755689846002\n",
            "Epoch # 336\n",
            "Loss =  0.4992725828500833\n",
            "Epoch # 337\n",
            "Loss =  0.5338338332044106\n",
            "Epoch # 338\n",
            "Loss =  0.5985664781722404\n",
            "Epoch # 339\n",
            "Loss =  0.7093223085701656\n",
            "Epoch # 340\n",
            "Loss =  1.0652639782837328\n",
            "Epoch # 341\n",
            "Loss =  2.203596064191063\n",
            "Epoch # 342\n",
            "Loss =  3.848668054741466\n",
            "Epoch # 343\n",
            "Loss =  3.1806245469579313\n",
            "Epoch # 344\n",
            "Loss =  1.0905097420966519\n",
            "Epoch # 345\n",
            "Loss =  1.0770182265340391\n",
            "Epoch # 346\n",
            "Loss =  1.0927506003019545\n",
            "Epoch # 347\n",
            "Loss =  1.320618812700916\n",
            "Epoch # 348\n",
            "Loss =  1.6379479768336949\n",
            "Epoch # 349\n",
            "Loss =  1.7550649390395552\n",
            "Epoch # 350\n",
            "Loss =  2.130114786193879\n",
            "Epoch # 351\n",
            "Loss =  1.2014167951358634\n",
            "Epoch # 352\n",
            "Loss =  0.8629070757497611\n",
            "Epoch # 353\n",
            "Loss =  0.6675951986719472\n",
            "Epoch # 354\n",
            "Loss =  0.5542603027214382\n",
            "Epoch # 355\n",
            "Loss =  0.5167708379989125\n",
            "Epoch # 356\n",
            "Loss =  0.4906185272462434\n",
            "Epoch # 357\n",
            "Loss =  0.4746066078393246\n",
            "Epoch # 358\n",
            "Loss =  0.4637802288628508\n",
            "Epoch # 359\n",
            "Loss =  0.4538464641070267\n",
            "Epoch # 360\n",
            "Loss =  0.44655185676075754\n",
            "Epoch # 361\n",
            "Loss =  0.43898470852516447\n",
            "Epoch # 362\n",
            "Loss =  0.4331326226227848\n",
            "Epoch # 363\n",
            "Loss =  0.4269525216064545\n",
            "Epoch # 364\n",
            "Loss =  0.422064830063433\n",
            "Epoch # 365\n",
            "Loss =  0.4168815042195585\n",
            "Epoch # 366\n",
            "Loss =  0.41280775595875846\n",
            "Epoch # 367\n",
            "Loss =  0.4084482676333153\n",
            "Epoch # 368\n",
            "Loss =  0.40517953662610223\n",
            "Epoch # 369\n",
            "Loss =  0.4016202946369826\n",
            "Epoch # 370\n",
            "Loss =  0.39931203578568664\n",
            "Epoch # 371\n",
            "Loss =  0.39674807116167654\n",
            "Epoch # 372\n",
            "Loss =  0.39585876131117353\n",
            "Epoch # 373\n",
            "Loss =  0.39489433383696493\n",
            "Epoch # 374\n",
            "Loss =  0.3963557969630374\n",
            "Epoch # 375\n",
            "Loss =  0.39824607383806737\n",
            "Epoch # 376\n",
            "Loss =  0.40371729263474965\n",
            "Epoch # 377\n",
            "Loss =  0.41062141884809655\n",
            "Epoch # 378\n",
            "Loss =  0.4235870473493942\n",
            "Epoch # 379\n",
            "Loss =  0.4389472533674959\n",
            "Epoch # 380\n",
            "Loss =  0.4670764699240989\n",
            "Epoch # 381\n",
            "Loss =  0.5001564990253853\n",
            "Epoch # 382\n",
            "Loss =  0.5664305810861245\n",
            "Epoch # 383\n",
            "Loss =  0.7041583711127274\n",
            "Epoch # 384\n",
            "Loss =  1.1558986377969398\n",
            "Epoch # 385\n",
            "Loss =  2.4008807897358597\n",
            "Epoch # 386\n",
            "Loss =  1.9472696769516762\n",
            "Epoch # 387\n",
            "Loss =  3.239394022635225\n",
            "Epoch # 388\n",
            "Loss =  2.5358737078712648\n",
            "Epoch # 389\n",
            "Loss =  1.3760787916847899\n",
            "Epoch # 390\n",
            "Loss =  0.8526230494053325\n",
            "Epoch # 391\n",
            "Loss =  0.8819306674239641\n",
            "Epoch # 392\n",
            "Loss =  0.7892063903359666\n",
            "Epoch # 393\n",
            "Loss =  0.7562608033828884\n",
            "Epoch # 394\n",
            "Loss =  0.6406715548380407\n",
            "Epoch # 395\n",
            "Loss =  0.5788386107155922\n",
            "Epoch # 396\n",
            "Loss =  0.4938086521828215\n",
            "Epoch # 397\n",
            "Loss =  0.4621874829300252\n",
            "Epoch # 398\n",
            "Loss =  0.4301829755006169\n",
            "Epoch # 399\n",
            "Loss =  0.4183539347435633\n",
            "Epoch # 400\n",
            "Loss =  0.40602357120139754\n",
            "Epoch # 401\n",
            "Loss =  0.3993254896851886\n",
            "Epoch # 402\n",
            "Loss =  0.3927203901268237\n",
            "Epoch # 403\n",
            "Loss =  0.38832438448327855\n",
            "Epoch # 404\n",
            "Loss =  0.38409374294660015\n",
            "Epoch # 405\n",
            "Loss =  0.38151629924577685\n",
            "Epoch # 406\n",
            "Loss =  0.3789158766704092\n",
            "Epoch # 407\n",
            "Loss =  0.3784399584373291\n",
            "Epoch # 408\n",
            "Loss =  0.37763442807998227\n",
            "Epoch # 409\n",
            "Loss =  0.38057907897777604\n",
            "Epoch # 410\n",
            "Loss =  0.3826537036701429\n",
            "Epoch # 411\n",
            "Loss =  0.392495153785941\n",
            "Epoch # 412\n",
            "Loss =  0.40207122874501194\n",
            "Epoch # 413\n",
            "Loss =  0.4265416944582006\n",
            "Epoch # 414\n",
            "Loss =  0.4595061339171175\n",
            "Epoch # 415\n",
            "Loss =  0.5205717190986023\n",
            "Epoch # 416\n",
            "Loss =  0.6387713596360992\n",
            "Epoch # 417\n",
            "Loss =  0.9251381198071332\n",
            "Epoch # 418\n",
            "Loss =  1.9118637404315582\n",
            "Epoch # 419\n",
            "Loss =  2.8807505743452837\n",
            "Epoch # 420\n",
            "Loss =  3.412195882362235\n",
            "Epoch # 421\n",
            "Loss =  3.899957835351905\n",
            "Epoch # 422\n",
            "Loss =  1.8308703524452823\n",
            "Epoch # 423\n",
            "Loss =  2.439848273308678\n",
            "Epoch # 424\n",
            "Loss =  2.7328542410214776\n",
            "Epoch # 425\n",
            "Loss =  0.9673655955472911\n",
            "Epoch # 426\n",
            "Loss =  0.5666183451282948\n",
            "Epoch # 427\n",
            "Loss =  0.50223884819037\n",
            "Epoch # 428\n",
            "Loss =  0.48158727015780506\n",
            "Epoch # 429\n",
            "Loss =  0.4635388910778389\n",
            "Epoch # 430\n",
            "Loss =  0.45266591009292767\n",
            "Epoch # 431\n",
            "Loss =  0.4414581018389452\n",
            "Epoch # 432\n",
            "Loss =  0.43303493878427246\n",
            "Epoch # 433\n",
            "Loss =  0.42446919794186017\n",
            "Epoch # 434\n",
            "Loss =  0.41750677133829533\n",
            "Epoch # 435\n",
            "Loss =  0.4105804807839637\n",
            "Epoch # 436\n",
            "Loss =  0.40474803209865257\n",
            "Epoch # 437\n",
            "Loss =  0.3990176431076433\n",
            "Epoch # 438\n",
            "Loss =  0.3941024025689211\n",
            "Epoch # 439\n",
            "Loss =  0.38935060627338247\n",
            "Epoch # 440\n",
            "Loss =  0.38531031333552307\n",
            "Epoch # 441\n",
            "Loss =  0.3815770243510154\n",
            "Epoch # 442\n",
            "Loss =  0.3786364213079036\n",
            "Epoch # 443\n",
            "Loss =  0.3764371076855213\n",
            "Epoch # 444\n",
            "Loss =  0.3752537288340099\n",
            "Epoch # 445\n",
            "Loss =  0.3759993831776446\n",
            "Epoch # 446\n",
            "Loss =  0.3777208405063566\n",
            "Epoch # 447\n",
            "Loss =  0.38400219995145973\n",
            "Epoch # 448\n",
            "Loss =  0.38967774806812683\n",
            "Epoch # 449\n",
            "Loss =  0.40517429455924237\n",
            "Epoch # 450\n",
            "Loss =  0.4155472257459587\n",
            "Epoch # 451\n",
            "Loss =  0.4456341026766895\n",
            "Epoch # 452\n",
            "Loss =  0.46577380421683373\n",
            "Epoch # 453\n",
            "Loss =  0.5233774925486732\n",
            "Epoch # 454\n",
            "Loss =  0.5815378683444979\n",
            "Epoch # 455\n",
            "Loss =  0.7191435729756872\n",
            "Epoch # 456\n",
            "Loss =  1.0089664713361952\n",
            "Epoch # 457\n",
            "Loss =  1.9171097965060435\n",
            "Epoch # 458\n",
            "Loss =  3.499644445410338\n",
            "Epoch # 459\n",
            "Loss =  3.175945531108392\n",
            "Epoch # 460\n",
            "Loss =  0.968071408401815\n",
            "Epoch # 461\n",
            "Loss =  0.8086672126946288\n",
            "Epoch # 462\n",
            "Loss =  0.7673788340365985\n",
            "Epoch # 463\n",
            "Loss =  0.6725923271049532\n",
            "Epoch # 464\n",
            "Loss =  0.5521644855791394\n",
            "Epoch # 465\n",
            "Loss =  0.4960171214612742\n",
            "Epoch # 466\n",
            "Loss =  0.42996821834397414\n",
            "Epoch # 467\n",
            "Loss =  0.4139910227798926\n",
            "Epoch # 468\n",
            "Loss =  0.39541211448104063\n",
            "Epoch # 469\n",
            "Loss =  0.38779739074484226\n",
            "Epoch # 470\n",
            "Loss =  0.37883421118212524\n",
            "Epoch # 471\n",
            "Loss =  0.3744881925927695\n",
            "Epoch # 472\n",
            "Loss =  0.3687208281274653\n",
            "Epoch # 473\n",
            "Loss =  0.3664404154633619\n",
            "Epoch # 474\n",
            "Loss =  0.36259615917947313\n",
            "Epoch # 475\n",
            "Loss =  0.3624484705415394\n",
            "Epoch # 476\n",
            "Loss =  0.36044370089780375\n",
            "Epoch # 477\n",
            "Loss =  0.36341490943521926\n",
            "Epoch # 478\n",
            "Loss =  0.36393550027837124\n",
            "Epoch # 479\n",
            "Loss =  0.3725445557163628\n",
            "Epoch # 480\n",
            "Loss =  0.37800883567106575\n",
            "Epoch # 481\n",
            "Loss =  0.397728958766018\n",
            "Epoch # 482\n",
            "Loss =  0.41527982147479225\n",
            "Epoch # 483\n",
            "Loss =  0.45747211561917955\n",
            "Epoch # 484\n",
            "Loss =  0.5114586396667589\n",
            "Epoch # 485\n",
            "Loss =  0.6251822279029119\n",
            "Epoch # 486\n",
            "Loss =  0.9254198586878283\n",
            "Epoch # 487\n",
            "Loss =  1.8220895836034248\n",
            "Epoch # 488\n",
            "Loss =  2.601284361299751\n",
            "Epoch # 489\n",
            "Loss =  2.1041968246870724\n",
            "Epoch # 490\n",
            "Loss =  3.63680592078709\n",
            "Epoch # 491\n",
            "Loss =  3.2223550149864746\n",
            "Epoch # 492\n",
            "Loss =  2.7823340230604647\n",
            "Epoch # 493\n",
            "Loss =  3.0870421769882057\n",
            "Epoch # 494\n",
            "Loss =  2.8893041395764056\n",
            "Epoch # 495\n",
            "Loss =  1.751455957325552\n",
            "Epoch # 496\n",
            "Loss =  0.9831858432223542\n",
            "Epoch # 497\n",
            "Loss =  0.915087799442263\n",
            "Epoch # 498\n",
            "Loss =  0.621679516614841\n",
            "Epoch # 499\n",
            "Loss =  0.5691018653683351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qObXUJoKf3ye",
        "colab_type": "code",
        "outputId": "58410c4e-d44b-4578-f72b-5a5b66acf94b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Problem 3 (Single hidden layer with N neurons)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip extreme values and thus prevents the loss from being infinite \n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_der(x):\n",
        "    return sigmoid(x) *(1-sigmoid (x))\n",
        "\n",
        "# Initialization\n",
        "def initialize_single_layer(X, hidden_layer_neurons):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights_hiddenlayer = np.random.randn(input_neurons,hidden_layer_neurons)\n",
        "    bias_hiddenlayer = np.random.randn(hidden_layer_neurons)\n",
        "    weights_outputlayer = np.random.randn(hidden_layer_neurons,output_neurons)\n",
        "    bias_outputlayer = np.random.randn(output_neurons)\n",
        "    weights = [weights_hiddenlayer,weights_outputlayer]\n",
        "    bias = [bias_hiddenlayer,bias_outputlayer]\n",
        "    return weights, bias    \n",
        "\n",
        "def update_weights_single_layer(X,Y,weights,bias,lr):\n",
        "    # Feedforward\n",
        "    # For the hidden layer\n",
        "    zhidden = np.dot(X,weights[0]) + bias[0]\n",
        "    ahidden = sigmoid(zhidden)\n",
        "    # For the output layer\n",
        "    zoutput = np.dot(ahidden,weights[1]) + bias[1]\n",
        "    aoutput = softmax(zoutput)\n",
        "    # Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    predictions = aoutput\n",
        "    # Printing the loss value\n",
        "    print(\"Loss = \",cross_entropy(predictions,target))\n",
        "    # For the output layer\n",
        "    dcost_dzo = predictions - target\n",
        "    dzo_dwo = ahidden\n",
        "    dcost_weights_output = np.dot(dzo_dwo.T,dcost_dzo)\n",
        "    dcost_bias_output = dcost_dzo\n",
        "    # For the hidden layer\n",
        "    # Arpit Omprakash\n",
        "    dzo_dah = weights[1]\n",
        "    dcost_dah = np.dot(dcost_dzo, dzo_dah.T)\n",
        "    dah_dzh = sigmoid_der(zhidden)\n",
        "    dcost_dzh = dah_dzh * dcost_dah\n",
        "    dzh_dwh = X\n",
        "    dcost_weights_hidden = np.dot(dzh_dwh.T, dcost_dzh)\n",
        "    dcost_bias_hidden = dcost_dzh\n",
        "    # Updating weights and biases\n",
        "    updated_weights = [weights[0]-lr*dcost_weights_hidden,weights[1]-lr*dcost_weights_output]\n",
        "    updated_bias = [bias[0]-lr*dcost_bias_hidden,bias[1]-lr*dcost_bias_output]\n",
        "    return updated_weights,updated_bias\n",
        "\n",
        "\n",
        "weights, bias = initialize_single_layer(X_train,hidden_layer_neurons = 12)\n",
        "for i in range(500):\n",
        "    print ('Epoch #',i)\n",
        "    weights,bias = update_weights_single_layer(X_train,Y_train,weights,bias,lr = 0.001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch # 0\n",
            "Loss =  4.146859067764598\n",
            "Epoch # 1\n",
            "Loss =  3.25044092154961\n",
            "Epoch # 2\n",
            "Loss =  2.853345183462456\n",
            "Epoch # 3\n",
            "Loss =  2.2549617295841324\n",
            "Epoch # 4\n",
            "Loss =  2.0559833118271995\n",
            "Epoch # 5\n",
            "Loss =  1.9721368909629775\n",
            "Epoch # 6\n",
            "Loss =  1.906973264465594\n",
            "Epoch # 7\n",
            "Loss =  1.8449523808913757\n",
            "Epoch # 8\n",
            "Loss =  1.782745540203513\n",
            "Epoch # 9\n",
            "Loss =  1.7199371734880338\n",
            "Epoch # 10\n",
            "Loss =  1.65780346639401\n",
            "Epoch # 11\n",
            "Loss =  1.5969200426845154\n",
            "Epoch # 12\n",
            "Loss =  1.5407369619789972\n",
            "Epoch # 13\n",
            "Loss =  1.488362407962956\n",
            "Epoch # 14\n",
            "Loss =  1.4396036211490912\n",
            "Epoch # 15\n",
            "Loss =  1.3939318806681535\n",
            "Epoch # 16\n",
            "Loss =  1.3505072149369541\n",
            "Epoch # 17\n",
            "Loss =  1.3098871964620782\n",
            "Epoch # 18\n",
            "Loss =  1.2715894810508355\n",
            "Epoch # 19\n",
            "Loss =  1.23886590336225\n",
            "Epoch # 20\n",
            "Loss =  1.2105584914109913\n",
            "Epoch # 21\n",
            "Loss =  1.1897211190575032\n",
            "Epoch # 22\n",
            "Loss =  1.1717848551722116\n",
            "Epoch # 23\n",
            "Loss =  1.1688322070178419\n",
            "Epoch # 24\n",
            "Loss =  1.1492156688092034\n",
            "Epoch # 25\n",
            "Loss =  1.1565026563999579\n",
            "Epoch # 26\n",
            "Loss =  1.1126359486619946\n",
            "Epoch # 27\n",
            "Loss =  1.105680896182494\n",
            "Epoch # 28\n",
            "Loss =  1.0646034918596308\n",
            "Epoch # 29\n",
            "Loss =  1.0493856787667897\n",
            "Epoch # 30\n",
            "Loss =  1.0179469643044121\n",
            "Epoch # 31\n",
            "Loss =  1.0046869945111656\n",
            "Epoch # 32\n",
            "Loss =  0.9757091471018282\n",
            "Epoch # 33\n",
            "Loss =  0.9651019021451533\n",
            "Epoch # 34\n",
            "Loss =  0.9369621881153295\n",
            "Epoch # 35\n",
            "Loss =  0.9292567626695929\n",
            "Epoch # 36\n",
            "Loss =  0.9017953737777434\n",
            "Epoch # 37\n",
            "Loss =  0.8956920090733216\n",
            "Epoch # 38\n",
            "Loss =  0.8692859931862866\n",
            "Epoch # 39\n",
            "Loss =  0.8628318188629852\n",
            "Epoch # 40\n",
            "Loss =  0.8390105898359514\n",
            "Epoch # 41\n",
            "Loss =  0.8317641470595305\n",
            "Epoch # 42\n",
            "Loss =  0.8119094474333594\n",
            "Epoch # 43\n",
            "Loss =  0.8043620681724527\n",
            "Epoch # 44\n",
            "Loss =  0.7881809589102375\n",
            "Epoch # 45\n",
            "Loss =  0.7805802198957017\n",
            "Epoch # 46\n",
            "Loss =  0.766993579240495\n",
            "Epoch # 47\n",
            "Loss =  0.7593302882700389\n",
            "Epoch # 48\n",
            "Loss =  0.7474592099545988\n",
            "Epoch # 49\n",
            "Loss =  0.7395911172487869\n",
            "Epoch # 50\n",
            "Loss =  0.728857847923544\n",
            "Epoch # 51\n",
            "Loss =  0.7206294052161624\n",
            "Epoch # 52\n",
            "Loss =  0.7107709166439309\n",
            "Epoch # 53\n",
            "Loss =  0.7021704688538075\n",
            "Epoch # 54\n",
            "Loss =  0.6931361919352429\n",
            "Epoch # 55\n",
            "Loss =  0.6843036869531463\n",
            "Epoch # 56\n",
            "Loss =  0.6760475977007342\n",
            "Epoch # 57\n",
            "Loss =  0.6671931854884667\n",
            "Epoch # 58\n",
            "Loss =  0.6595832159738642\n",
            "Epoch # 59\n",
            "Loss =  0.6509813786842592\n",
            "Epoch # 60\n",
            "Loss =  0.6438028968554746\n",
            "Epoch # 61\n",
            "Loss =  0.6356722289260459\n",
            "Epoch # 62\n",
            "Loss =  0.6287461690896621\n",
            "Epoch # 63\n",
            "Loss =  0.6211467977889542\n",
            "Epoch # 64\n",
            "Loss =  0.6143761663735312\n",
            "Epoch # 65\n",
            "Loss =  0.607252491502891\n",
            "Epoch # 66\n",
            "Loss =  0.600633031457163\n",
            "Epoch # 67\n",
            "Loss =  0.5939266036923432\n",
            "Epoch # 68\n",
            "Loss =  0.5875309219442729\n",
            "Epoch # 69\n",
            "Loss =  0.5812024742855126\n",
            "Epoch # 70\n",
            "Loss =  0.5750866622656402\n",
            "Epoch # 71\n",
            "Loss =  0.5690742521169734\n",
            "Epoch # 72\n",
            "Loss =  0.5632076606660448\n",
            "Epoch # 73\n",
            "Loss =  0.5574388880694121\n",
            "Epoch # 74\n",
            "Loss =  0.5517943246511403\n",
            "Epoch # 75\n",
            "Loss =  0.546320426320242\n",
            "Epoch # 76\n",
            "Loss =  0.5410081751774893\n",
            "Epoch # 77\n",
            "Loss =  0.5359246436621025\n",
            "Epoch # 78\n",
            "Loss =  0.5309824371112024\n",
            "Epoch # 79\n",
            "Loss =  0.5262664716577351\n",
            "Epoch # 80\n",
            "Loss =  0.5216577826156961\n",
            "Epoch # 81\n",
            "Loss =  0.5172630623185293\n",
            "Epoch # 82\n",
            "Loss =  0.5129454088547715\n",
            "Epoch # 83\n",
            "Loss =  0.5088279568557594\n",
            "Epoch # 84\n",
            "Loss =  0.5047639773564376\n",
            "Epoch # 85\n",
            "Loss =  0.5008819754224189\n",
            "Epoch # 86\n",
            "Loss =  0.49703350262143087\n",
            "Epoch # 87\n",
            "Loss =  0.4933423569601541\n",
            "Epoch # 88\n",
            "Loss =  0.48967050739076823\n",
            "Epoch # 89\n",
            "Loss =  0.4861268807993358\n",
            "Epoch # 90\n",
            "Loss =  0.48259942001651024\n",
            "Epoch # 91\n",
            "Loss =  0.4791683953034071\n",
            "Epoch # 92\n",
            "Loss =  0.475764126538259\n",
            "Epoch # 93\n",
            "Loss =  0.47242656036847575\n",
            "Epoch # 94\n",
            "Loss =  0.46913692591131184\n",
            "Epoch # 95\n",
            "Loss =  0.46589297036830685\n",
            "Epoch # 96\n",
            "Loss =  0.4627179284078307\n",
            "Epoch # 97\n",
            "Loss =  0.4595788917607156\n",
            "Epoch # 98\n",
            "Loss =  0.4565187171553309\n",
            "Epoch # 99\n",
            "Loss =  0.45349425250612857\n",
            "Epoch # 100\n",
            "Loss =  0.450546427547198\n",
            "Epoch # 101\n",
            "Loss =  0.447639351245951\n",
            "Epoch # 102\n",
            "Loss =  0.4448017561847348\n",
            "Epoch # 103\n",
            "Loss =  0.44201197254191\n",
            "Epoch # 104\n",
            "Loss =  0.43928683252268946\n",
            "Epoch # 105\n",
            "Loss =  0.4366160177539008\n",
            "Epoch # 106\n",
            "Loss =  0.4340071087103863\n",
            "Epoch # 107\n",
            "Loss =  0.4314542914890095\n",
            "Epoch # 108\n",
            "Loss =  0.4289586430199555\n",
            "Epoch # 109\n",
            "Loss =  0.4265154194069578\n",
            "Epoch # 110\n",
            "Loss =  0.4241228160959079\n",
            "Epoch # 111\n",
            "Loss =  0.42177686224643984\n",
            "Epoch # 112\n",
            "Loss =  0.41947529223989166\n",
            "Epoch # 113\n",
            "Loss =  0.41721516061666036\n",
            "Epoch # 114\n",
            "Loss =  0.41499472332846254\n",
            "Epoch # 115\n",
            "Loss =  0.41281206933280173\n",
            "Epoch # 116\n",
            "Loss =  0.4106661281496783\n",
            "Epoch # 117\n",
            "Loss =  0.4085557336589345\n",
            "Epoch # 118\n",
            "Loss =  0.4064802396528665\n",
            "Epoch # 119\n",
            "Loss =  0.40443881542071586\n",
            "Epoch # 120\n",
            "Loss =  0.4024309069056092\n",
            "Epoch # 121\n",
            "Loss =  0.4004557155808582\n",
            "Epoch # 122\n",
            "Loss =  0.39851257586963057\n",
            "Epoch # 123\n",
            "Loss =  0.3966006105550061\n",
            "Epoch # 124\n",
            "Loss =  0.39471899960434853\n",
            "Epoch # 125\n",
            "Loss =  0.3928667983172155\n",
            "Epoch # 126\n",
            "Loss =  0.39104307090969326\n",
            "Epoch # 127\n",
            "Loss =  0.3892468381567487\n",
            "Epoch # 128\n",
            "Loss =  0.387477090716201\n",
            "Epoch # 129\n",
            "Loss =  0.3857328219137816\n",
            "Epoch # 130\n",
            "Loss =  0.3840129693189066\n",
            "Epoch # 131\n",
            "Loss =  0.3823164927535316\n",
            "Epoch # 132\n",
            "Loss =  0.3806422971995468\n",
            "Epoch # 133\n",
            "Loss =  0.3789893309344167\n",
            "Epoch # 134\n",
            "Loss =  0.37735651917212965\n",
            "Epoch # 135\n",
            "Loss =  0.37574287949955765\n",
            "Epoch # 136\n",
            "Loss =  0.37414745318313763\n",
            "Epoch # 137\n",
            "Loss =  0.37256947026511944\n",
            "Epoch # 138\n",
            "Loss =  0.3710082040157976\n",
            "Epoch # 139\n",
            "Loss =  0.3694632835905526\n",
            "Epoch # 140\n",
            "Loss =  0.3679342897561286\n",
            "Epoch # 141\n",
            "Loss =  0.36642146149435423\n",
            "Epoch # 142\n",
            "Loss =  0.3649246300094554\n",
            "Epoch # 143\n",
            "Loss =  0.3634449246151734\n",
            "Epoch # 144\n",
            "Loss =  0.3619821428643002\n",
            "Epoch # 145\n",
            "Loss =  0.36053887929389145\n",
            "Epoch # 146\n",
            "Loss =  0.3591142527697705\n",
            "Epoch # 147\n",
            "Loss =  0.35771368098528766\n",
            "Epoch # 148\n",
            "Loss =  0.3563341955580701\n",
            "Epoch # 149\n",
            "Loss =  0.3549870506349378\n",
            "Epoch # 150\n",
            "Loss =  0.3536639853443304\n",
            "Epoch # 151\n",
            "Loss =  0.35238835623864556\n",
            "Epoch # 152\n",
            "Loss =  0.35113949656658927\n",
            "Epoch # 153\n",
            "Loss =  0.3499650342054338\n",
            "Epoch # 154\n",
            "Loss =  0.34881726487638814\n",
            "Epoch # 155\n",
            "Loss =  0.3477889893778845\n",
            "Epoch # 156\n",
            "Loss =  0.34677908253790074\n",
            "Epoch # 157\n",
            "Loss =  0.3459536365372966\n",
            "Epoch # 158\n",
            "Loss =  0.3451198801110812\n",
            "Epoch # 159\n",
            "Loss =  0.344539660609338\n",
            "Epoch # 160\n",
            "Loss =  0.34388855527298445\n",
            "Epoch # 161\n",
            "Loss =  0.34353033269545946\n",
            "Epoch # 162\n",
            "Loss =  0.34298951412464257\n",
            "Epoch # 163\n",
            "Loss =  0.34275047878974063\n",
            "Epoch # 164\n",
            "Loss =  0.3422234346212899\n",
            "Epoch # 165\n",
            "Loss =  0.34200486247580336\n",
            "Epoch # 166\n",
            "Loss =  0.3414920768938719\n",
            "Epoch # 167\n",
            "Loss =  0.34120288152257633\n",
            "Epoch # 168\n",
            "Loss =  0.34070689298215917\n",
            "Epoch # 169\n",
            "Loss =  0.34015627901912926\n",
            "Epoch # 170\n",
            "Loss =  0.3395571671191785\n",
            "Epoch # 171\n",
            "Loss =  0.3385625036812079\n",
            "Epoch # 172\n",
            "Loss =  0.33775371864369996\n",
            "Epoch # 173\n",
            "Loss =  0.33632308474066996\n",
            "Epoch # 174\n",
            "Loss =  0.33531146444509674\n",
            "Epoch # 175\n",
            "Loss =  0.3335947003338112\n",
            "Epoch # 176\n",
            "Loss =  0.332446334774276\n",
            "Epoch # 177\n",
            "Loss =  0.3306110709787357\n",
            "Epoch # 178\n",
            "Loss =  0.3293941210078393\n",
            "Epoch # 179\n",
            "Loss =  0.3275702433854462\n",
            "Epoch # 180\n",
            "Loss =  0.3263377358710456\n",
            "Epoch # 181\n",
            "Loss =  0.32461128389838473\n",
            "Epoch # 182\n",
            "Loss =  0.3234026998493207\n",
            "Epoch # 183\n",
            "Loss =  0.32182010045584514\n",
            "Epoch # 184\n",
            "Loss =  0.32066342120051544\n",
            "Epoch # 185\n",
            "Loss =  0.3192369971918006\n",
            "Epoch # 186\n",
            "Loss =  0.31814787224763075\n",
            "Epoch # 187\n",
            "Loss =  0.31686591078985205\n",
            "Epoch # 188\n",
            "Loss =  0.315848443604307\n",
            "Epoch # 189\n",
            "Loss =  0.3146878218821589\n",
            "Epoch # 190\n",
            "Loss =  0.3137385366597455\n",
            "Epoch # 191\n",
            "Loss =  0.3126745746981744\n",
            "Epoch # 192\n",
            "Loss =  0.3117869529983744\n",
            "Epoch # 193\n",
            "Loss =  0.3107983141034389\n",
            "Epoch # 194\n",
            "Loss =  0.3099657318402949\n",
            "Epoch # 195\n",
            "Loss =  0.3090355678511965\n",
            "Epoch # 196\n",
            "Loss =  0.3082524604890377\n",
            "Epoch # 197\n",
            "Loss =  0.3073677499838389\n",
            "Epoch # 198\n",
            "Loss =  0.3066296834465819\n",
            "Epoch # 199\n",
            "Loss =  0.30578005811691683\n",
            "Epoch # 200\n",
            "Loss =  0.3050833066430087\n",
            "Epoch # 201\n",
            "Loss =  0.3042599597307368\n",
            "Epoch # 202\n",
            "Loss =  0.30360093444691644\n",
            "Epoch # 203\n",
            "Loss =  0.3027958330912302\n",
            "Epoch # 204\n",
            "Loss =  0.3021705471778692\n",
            "Epoch # 205\n",
            "Loss =  0.3013760324248942\n",
            "Epoch # 206\n",
            "Loss =  0.30077972918968576\n",
            "Epoch # 207\n",
            "Loss =  0.29998852991645975\n",
            "Epoch # 208\n",
            "Loss =  0.2994155742133538\n",
            "Epoch # 209\n",
            "Loss =  0.2986211910818712\n",
            "Epoch # 210\n",
            "Loss =  0.2980652792102926\n",
            "Epoch # 211\n",
            "Loss =  0.29726259256810506\n",
            "Epoch # 212\n",
            "Loss =  0.2967172515187782\n",
            "Epoch # 213\n",
            "Loss =  0.2959031171859285\n",
            "Epoch # 214\n",
            "Loss =  0.2953623637880198\n",
            "Epoch # 215\n",
            "Loss =  0.2945359502918591\n",
            "Epoch # 216\n",
            "Loss =  0.2939949161636118\n",
            "Epoch # 217\n",
            "Loss =  0.2931576338568886\n",
            "Epoch # 218\n",
            "Loss =  0.29261297465987307\n",
            "Epoch # 219\n",
            "Loss =  0.2917680067290698\n",
            "Epoch # 220\n",
            "Loss =  0.29121800079411025\n",
            "Epoch # 221\n",
            "Loss =  0.2903695842127531\n",
            "Epoch # 222\n",
            "Loss =  0.2898139392495345\n",
            "Epoch # 223\n",
            "Loss =  0.2889665994204797\n",
            "Epoch # 224\n",
            "Loss =  0.28840607060513473\n",
            "Epoch # 225\n",
            "Loss =  0.28756398252471593\n",
            "Epoch # 226\n",
            "Loss =  0.28699993100033505\n",
            "Epoch # 227\n",
            "Loss =  0.28616650036285285\n",
            "Epoch # 228\n",
            "Loss =  0.28560049518953795\n",
            "Epoch # 229\n",
            "Loss =  0.28477817083623946\n",
            "Epoch # 230\n",
            "Loss =  0.2842116886588852\n",
            "Epoch # 231\n",
            "Loss =  0.28340196232336246\n",
            "Epoch # 232\n",
            "Loss =  0.28283619560685946\n",
            "Epoch # 233\n",
            "Loss =  0.2820397232019794\n",
            "Epoch # 234\n",
            "Loss =  0.2814754835613355\n",
            "Epoch # 235\n",
            "Loss =  0.28069226676895775\n",
            "Epoch # 236\n",
            "Loss =  0.28012996402065965\n",
            "Epoch # 237\n",
            "Loss =  0.27935954934050944\n",
            "Epoch # 238\n",
            "Loss =  0.27879923163539694\n",
            "Epoch # 239\n",
            "Loss =  0.27804090589471664\n",
            "Epoch # 240\n",
            "Loss =  0.27748235383677844\n",
            "Epoch # 241\n",
            "Loss =  0.27673533449146454\n",
            "Epoch # 242\n",
            "Loss =  0.27617820811753474\n",
            "Epoch # 243\n",
            "Loss =  0.27544184183128034\n",
            "Epoch # 244\n",
            "Loss =  0.27488588314034307\n",
            "Epoch # 245\n",
            "Loss =  0.2741598788223423\n",
            "Epoch # 246\n",
            "Loss =  0.27360517618933344\n",
            "Epoch # 247\n",
            "Loss =  0.27288991356587167\n",
            "Epoch # 248\n",
            "Loss =  0.27233724085624217\n",
            "Epoch # 249\n",
            "Loss =  0.27163422106578783\n",
            "Epoch # 250\n",
            "Loss =  0.27108547615866024\n",
            "Epoch # 251\n",
            "Loss =  0.2703980298487987\n",
            "Epoch # 252\n",
            "Loss =  0.2698568114505652\n",
            "Epoch # 253\n",
            "Loss =  0.2691912658125572\n",
            "Epoch # 254\n",
            "Loss =  0.2686636072355547\n",
            "Epoch # 255\n",
            "Loss =  0.2680312191352624\n",
            "Epoch # 256\n",
            "Loss =  0.2675262726970735\n",
            "Epoch # 257\n",
            "Loss =  0.2669462027769216\n",
            "Epoch # 258\n",
            "Loss =  0.26647569733540044\n",
            "Epoch # 259\n",
            "Loss =  0.2659785830257323\n",
            "Epoch # 260\n",
            "Loss =  0.26555112543916043\n",
            "Epoch # 261\n",
            "Loss =  0.26518029773420454\n",
            "Epoch # 262\n",
            "Loss =  0.2647826532819674\n",
            "Epoch # 263\n",
            "Loss =  0.26458517804717124\n",
            "Epoch # 264\n",
            "Loss =  0.26414912712765104\n",
            "Epoch # 265\n",
            "Loss =  0.26414661342854034\n",
            "Epoch # 266\n",
            "Loss =  0.2635394993500832\n",
            "Epoch # 267\n",
            "Loss =  0.2636863424500715\n",
            "Epoch # 268\n",
            "Loss =  0.2627866926680224\n",
            "Epoch # 269\n",
            "Loss =  0.2629629331866639\n",
            "Epoch # 270\n",
            "Loss =  0.26176672356872677\n",
            "Epoch # 271\n",
            "Loss =  0.2618372451572175\n",
            "Epoch # 272\n",
            "Loss =  0.2604607087857996\n",
            "Epoch # 273\n",
            "Loss =  0.2603473726347552\n",
            "Epoch # 274\n",
            "Loss =  0.2589391166143874\n",
            "Epoch # 275\n",
            "Loss =  0.25864156871296173\n",
            "Epoch # 276\n",
            "Loss =  0.25730815438612165\n",
            "Epoch # 277\n",
            "Loss =  0.2568769487225209\n",
            "Epoch # 278\n",
            "Loss =  0.2556645965564218\n",
            "Epoch # 279\n",
            "Loss =  0.2551658421858008\n",
            "Epoch # 280\n",
            "Loss =  0.2540774208754648\n",
            "Epoch # 281\n",
            "Loss =  0.25357109788227405\n",
            "Epoch # 282\n",
            "Loss =  0.2525879610983049\n",
            "Epoch # 283\n",
            "Loss =  0.25211917355644503\n",
            "Epoch # 284\n",
            "Loss =  0.251215572612678\n",
            "Epoch # 285\n",
            "Loss =  0.25081262600750953\n",
            "Epoch # 286\n",
            "Loss =  0.24996359558392314\n",
            "Epoch # 287\n",
            "Loss =  0.24963947087299426\n",
            "Epoch # 288\n",
            "Loss =  0.24882492040927479\n",
            "Epoch # 289\n",
            "Loss =  0.24858049372489854\n",
            "Epoch # 290\n",
            "Loss =  0.24778672363903542\n",
            "Epoch # 291\n",
            "Loss =  0.2476144620626022\n",
            "Epoch # 292\n",
            "Loss =  0.24683358371538078\n",
            "Epoch # 293\n",
            "Loss =  0.2467207696909843\n",
            "Epoch # 294\n",
            "Loss =  0.24594861666808474\n",
            "Epoch # 295\n",
            "Loss =  0.24587969802011134\n",
            "Epoch # 296\n",
            "Loss =  0.24511306315864162\n",
            "Epoch # 297\n",
            "Loss =  0.24507117760368996\n",
            "Epoch # 298\n",
            "Loss =  0.2443053124505519\n",
            "Epoch # 299\n",
            "Loss =  0.24427321813502925\n",
            "Epoch # 300\n",
            "Loss =  0.24350053616041392\n",
            "Epoch # 301\n",
            "Loss =  0.24346122434369163\n",
            "Epoch # 302\n",
            "Loss =  0.24267195984751733\n",
            "Epoch # 303\n",
            "Loss =  0.24260933481850716\n",
            "Epoch # 304\n",
            "Loss =  0.24179423928641913\n",
            "Epoch # 305\n",
            "Loss =  0.24169432326927062\n",
            "Epoch # 306\n",
            "Loss =  0.2408482858398541\n",
            "Epoch # 307\n",
            "Loss =  0.2407010663595146\n",
            "Epoch # 308\n",
            "Loss =  0.23982556493296\n",
            "Epoch # 309\n",
            "Loss =  0.2396269466799436\n",
            "Epoch # 310\n",
            "Loss =  0.23872975829968562\n",
            "Epoch # 311\n",
            "Loss =  0.23848284426373845\n",
            "Epoch # 312\n",
            "Loss =  0.2375754165742321\n",
            "Epoch # 313\n",
            "Loss =  0.23729064883927511\n",
            "Epoch # 314\n",
            "Loss =  0.23638485938719186\n",
            "Epoch # 315\n",
            "Loss =  0.2360789223125938\n",
            "Epoch # 316\n",
            "Loss =  0.2351845407059923\n",
            "Epoch # 317\n",
            "Loss =  0.23487833944376627\n",
            "Epoch # 318\n",
            "Loss =  0.23400150050840243\n",
            "Epoch # 319\n",
            "Loss =  0.23371784761177386\n",
            "Epoch # 320\n",
            "Loss =  0.2328601845346483\n",
            "Epoch # 321\n",
            "Loss =  0.23262165338576957\n",
            "Epoch # 322\n",
            "Loss =  0.23177956941880226\n",
            "Epoch # 323\n",
            "Loss =  0.23160629352050804\n",
            "Epoch # 324\n",
            "Loss =  0.230770358623522\n",
            "Epoch # 325\n",
            "Loss =  0.23067704205956202\n",
            "Epoch # 326\n",
            "Loss =  0.22983263860422176\n",
            "Epoch # 327\n",
            "Loss =  0.22982473452933633\n",
            "Epoch # 328\n",
            "Loss =  0.22895556159408634\n",
            "Epoch # 329\n",
            "Loss =  0.22902631685625133\n",
            "Epoch # 330\n",
            "Loss =  0.22812041808302055\n",
            "Epoch # 331\n",
            "Loss =  0.22825106562266761\n",
            "Epoch # 332\n",
            "Loss =  0.22730605447271782\n",
            "Epoch # 333\n",
            "Loss =  0.22746960992874424\n",
            "Epoch # 334\n",
            "Loss =  0.22649381412615124\n",
            "Epoch # 335\n",
            "Loss =  0.22666067733509004\n",
            "Epoch # 336\n",
            "Loss =  0.22567010237406088\n",
            "Epoch # 337\n",
            "Loss =  0.22581311787840913\n",
            "Epoch # 338\n",
            "Loss =  0.2248266140803983\n",
            "Epoch # 339\n",
            "Loss =  0.22492420898899773\n",
            "Epoch # 340\n",
            "Loss =  0.22395934903008397\n",
            "Epoch # 341\n",
            "Loss =  0.22399662265123024\n",
            "Epoch # 342\n",
            "Loss =  0.22306761612447318\n",
            "Epoch # 343\n",
            "Loss =  0.22303604593666218\n",
            "Epoch # 344\n",
            "Loss =  0.22215375146850316\n",
            "Epoch # 345\n",
            "Loss =  0.22205034451043562\n",
            "Epoch # 346\n",
            "Loss =  0.22122364257496155\n",
            "Epoch # 347\n",
            "Loss =  0.22105007038293248\n",
            "Epoch # 348\n",
            "Loss =  0.2202876231031426\n",
            "Epoch # 349\n",
            "Loss =  0.22004942066265043\n",
            "Epoch # 350\n",
            "Loss =  0.21936100367811823\n",
            "Epoch # 351\n",
            "Loss =  0.21906654342664733\n",
            "Epoch # 352\n",
            "Loss =  0.21846344502849907\n",
            "Epoch # 353\n",
            "Loss =  0.2181222767489413\n",
            "Epoch # 354\n",
            "Loss =  0.2176165368869937\n",
            "Epoch # 355\n",
            "Loss =  0.2172369517516429\n",
            "Epoch # 356\n",
            "Loss =  0.21683942923745486\n",
            "Epoch # 357\n",
            "Loss =  0.21642586551075368\n",
            "Epoch # 358\n",
            "Loss =  0.21614340224833273\n",
            "Epoch # 359\n",
            "Loss =  0.2156950160103237\n",
            "Epoch # 360\n",
            "Loss =  0.2155272819943322\n",
            "Epoch # 361\n",
            "Loss =  0.21503822715448115\n",
            "Epoch # 362\n",
            "Loss =  0.21497479670657316\n",
            "Epoch # 363\n",
            "Loss =  0.21443499134698196\n",
            "Epoch # 364\n",
            "Loss =  0.21445289050003144\n",
            "Epoch # 365\n",
            "Loss =  0.21384874029809808\n",
            "Epoch # 366\n",
            "Loss =  0.2139111221141844\n",
            "Epoch # 367\n",
            "Loss =  0.21322860399751048\n",
            "Epoch # 368\n",
            "Loss =  0.2132867566275657\n",
            "Epoch # 369\n",
            "Loss =  0.2125193648749058\n",
            "Epoch # 370\n",
            "Loss =  0.21252119555581786\n",
            "Epoch # 371\n",
            "Loss =  0.21168060875579667\n",
            "Epoch # 372\n",
            "Loss =  0.21158591336473528\n",
            "Epoch # 373\n",
            "Loss =  0.2107080677609923\n",
            "Epoch # 374\n",
            "Loss =  0.21050341845788798\n",
            "Epoch # 375\n",
            "Loss =  0.20964244066496307\n",
            "Epoch # 376\n",
            "Loss =  0.20934349582516848\n",
            "Epoch # 377\n",
            "Loss =  0.2085522348294638\n",
            "Epoch # 378\n",
            "Loss =  0.20819051383828238\n",
            "Epoch # 379\n",
            "Loss =  0.20749874724865772\n",
            "Epoch # 380\n",
            "Loss =  0.20710651334184868\n",
            "Epoch # 381\n",
            "Loss =  0.20651502818752251\n",
            "Epoch # 382\n",
            "Loss =  0.20611999800210443\n",
            "Epoch # 383\n",
            "Loss =  0.20561206130868023\n",
            "Epoch # 384\n",
            "Loss =  0.20523871740270083\n",
            "Epoch # 385\n",
            "Loss =  0.20479295934429093\n",
            "Epoch # 386\n",
            "Loss =  0.2044638695175812\n",
            "Epoch # 387\n",
            "Loss =  0.2040598726295542\n",
            "Epoch # 388\n",
            "Loss =  0.20379494616422034\n",
            "Epoch # 389\n",
            "Loss =  0.2034136301770382\n",
            "Epoch # 390\n",
            "Loss =  0.2032276680574848\n",
            "Epoch # 391\n",
            "Loss =  0.20285018269458055\n",
            "Epoch # 392\n",
            "Loss =  0.20274954903043865\n",
            "Epoch # 393\n",
            "Loss =  0.2023567880202153\n",
            "Epoch # 394\n",
            "Loss =  0.20233635689919738\n",
            "Epoch # 395\n",
            "Loss =  0.2019098951708918\n",
            "Epoch # 396\n",
            "Loss =  0.20195166354107463\n",
            "Epoch # 397\n",
            "Loss =  0.20147622953103875\n",
            "Epoch # 398\n",
            "Loss =  0.20155073421307854\n",
            "Epoch # 399\n",
            "Loss =  0.20101783501425016\n",
            "Epoch # 400\n",
            "Loss =  0.2010888039171395\n",
            "Epoch # 401\n",
            "Loss =  0.20050015879656813\n",
            "Epoch # 402\n",
            "Loss =  0.20053173564495877\n",
            "Epoch # 403\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAHyRXd1g7l3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 4 (Two hidden layers with N neurons each)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip extreme values and thus prevents the loss from being infinite \n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_der(x):\n",
        "    return sigmoid(x) *(1-sigmoid (x))\n",
        "\n",
        "# Initialization\n",
        "def initialize_double_layer(X, hidden_layer_neurons):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights_h1 = np.random.randn(input_neurons,hidden_layer_neurons)\n",
        "    bias_h1 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_h2 = np.random.randn(hidden_layer_neurons,hidden_layer_neurons)\n",
        "    bias_h2 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_o = np.random.randn(hidden_layer_neurons,output_neurons)\n",
        "    bias_o = np.random.randn(output_neurons)\n",
        "    weights = [weights_h1, weights_h2,weights_o]\n",
        "    bias = [bias_h1, bias_h2, bias_o]\n",
        "    return weights, bias\n",
        "\n",
        "def update_weights_double_layer(X,Y,weights,bias,lr):\n",
        "    # Feedforward\n",
        "    # For h1 layer (hidden)\n",
        "    zh1 = np.dot(X,weights[0]) + bias[0]\n",
        "    ah1 = sigmoid(zh1)\n",
        "    # For h2 layer(hidden)\n",
        "    zh2 = np.dot(ah1,weights[1]) + bias[1]\n",
        "    ah2 = sigmoid(zh2)\n",
        "    # For output layer\n",
        "    zo = np.dot(ah2,weights[2]) + bias[2]\n",
        "    ao = softmax(zo)\n",
        "    # Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    predictions = ao\n",
        "    # Printing loss value\n",
        "    # Arpit Omprakash\n",
        "    print(\"Loss: \", cross_entropy(predictions,target))\n",
        "    # For the output layer\n",
        "    dcost_dzo = predictions - target\n",
        "    dzo_dwo = ah2\n",
        "    dcost_weights_o = np.dot(dzo_dwo.T,dcost_dzo)\n",
        "    dcost_bias_o = dcost_dzo\n",
        "    # For h2 layer (hidden)\n",
        "    dzo_dah2 = weights[2]\n",
        "    dcost_dah2 = np.dot(dcost_dzo , dzo_dah2.T)\n",
        "    dah2_dzh2 = sigmoid_der(zh2)\n",
        "    dcost_dzh2 = dah2_dzh2 * dcost_dah2\n",
        "    dzh2_dwh2 = ah1\n",
        "    dcost_weights_h2 = np.dot(dzh2_dwh2.T, dcost_dzh2)\n",
        "    dcost_bias_h2 = dcost_dzh2\n",
        "    # For h1 layer (hidden)\n",
        "    dzh2_dah1 = weights[1]\n",
        "    dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)\n",
        "    dah1_dzh1 = sigmoid_der(zh1)\n",
        "    dcost_dzh1 = dah1_dzh1 * dcost_dah1\n",
        "    dzh1_dwh1 = X\n",
        "    dcost_weights_h1 = np.dot(dzh1_dwh1.T, dcost_dzh1)\n",
        "    dcost_bias_h1 = dcost_dzh1\n",
        "    # Updating weights and biases\n",
        "    updated_weights = [weights[0]-lr*dcost_weights_h1,weights[1]-lr*dcost_weights_h2,weights[2]-lr*dcost_weights_o]\n",
        "    updated_bias = [bias[0]-lr*dcost_bias_h1,bias[1]-lr*dcost_bias_h2,bias[2]-lr*dcost_bias_o]\n",
        "    return updated_weights,updated_bias\n",
        "\n",
        "weights, bias = initialize_double_layer(X_train, hidden_layer_neurons = 10)\n",
        "for i in range(500):\n",
        "    print ('Epoch #',i)\n",
        "    weights,bias = update_weights_double_layer(X_train,Y_train,weights,bias,lr = 0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhaZIijiqI6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 5 (Two hidden layers with activation function)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip extreme values and thus prevents the loss from being infinite \n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_der(x):\n",
        "    return sigmoid(x) *(1-sigmoid (x))\n",
        "\n",
        "# TanH function\n",
        "def tanh(x):\n",
        "    temp = 2/(1+np.exp(-2*x))\n",
        "    return temp - 1\n",
        "\n",
        "# Derivative of TanH function\n",
        "def tanh_der(x):\n",
        "    return 1 - (tanh(x)*tanh(x))\n",
        "\n",
        "# ReLU function\n",
        "def relu(x):\n",
        "    return x * (x > 0) \n",
        "\n",
        "# Derivative of ReLU function\n",
        "def relu_der(x):\n",
        "    return 1. * (x > 0)\n",
        "\n",
        "# References to functions and derivatives\n",
        "f1 = sigmoid\n",
        "f2 = tanh\n",
        "f3 = relu\n",
        "f = [f1, f2, f3]\n",
        "d1 = sigmoid_der\n",
        "d2 = tanh_der\n",
        "d3 = relu_der\n",
        "d = [d1, d2, d3]\n",
        "\n",
        "# Exception class for any error in the parameters provided\n",
        "class ParameterError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Initialization\n",
        "def initialize_double_layer_act(X, hidden_layer_neurons):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights_h1 = np.random.randn(input_neurons,hidden_layer_neurons)\n",
        "    bias_h1 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_h2 = np.random.randn(hidden_layer_neurons,hidden_layer_neurons)\n",
        "    bias_h2 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_o = np.random.randn(hidden_layer_neurons,output_neurons)\n",
        "    bias_o = np.random.randn(output_neurons)\n",
        "    weights = [weights_h1, weights_h2,weights_o]\n",
        "    bias = [bias_h1, bias_h2, bias_o]\n",
        "    return weights, bias\n",
        "\n",
        "def update_weights_double_layer_act(X,Y,weights,bias,lr,activation):\n",
        "    # Changing activation function according to the parameter\n",
        "    if activation == 'sigmoid':\n",
        "        func = f1\n",
        "        deriv = d1\n",
        "    elif activation == 'tanh':\n",
        "        func = f2\n",
        "        deriv = d2\n",
        "    elif activation == 'relu':\n",
        "        func = f3\n",
        "        deriv = d3\n",
        "    else:\n",
        "        print(\"Invalid activation function\")\n",
        "        raise ParameterError('Activation function incorrect')\n",
        "    # Feedforward\n",
        "    # For h1 layer (hidden)\n",
        "    zh1 = np.dot(X,weights[0]) + bias[0]\n",
        "    ah1 = func(zh1)\n",
        "    # For h2 layer(hidden)\n",
        "    zh2 = np.dot(ah1,weights[1]) + bias[1]\n",
        "    ah2 = func(zh2)\n",
        "    # For output layer\n",
        "    zo = np.dot(ah2,weights[2]) + bias[2]\n",
        "    ao = softmax(zo)\n",
        "    # Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    predictions = ao\n",
        "    # Printing loss value\n",
        "    print(\"Loss: \", cross_entropy(predictions,target))\n",
        "    # For the output layer\n",
        "    # Arpit Omprakash\n",
        "    dcost_dzo = predictions - target\n",
        "    dzo_dwo = ah2\n",
        "    dcost_weights_o = np.dot(dzo_dwo.T,dcost_dzo)\n",
        "    dcost_bias_o = dcost_dzo\n",
        "    # For h2 layer (hidden)\n",
        "    dzo_dah2 = weights[2]\n",
        "    dcost_dah2 = np.dot(dcost_dzo , dzo_dah2.T)\n",
        "    dah2_dzh2 = deriv(zh2)\n",
        "    dcost_dzh2 = dah2_dzh2 * dcost_dah2\n",
        "    dzh2_dwh2 = ah1\n",
        "    dcost_weights_h2 = np.dot(dzh2_dwh2.T, dcost_dzh2)\n",
        "    dcost_bias_h2 = dcost_dzh2\n",
        "    # For h1 layer (hidden)\n",
        "    dzh2_dah1 = weights[1]\n",
        "    dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)\n",
        "    dah1_dzh1 = deriv(zh1)\n",
        "    dcost_dzh1 = dah1_dzh1 * dcost_dah1\n",
        "    dzh1_dwh1 = X\n",
        "    dcost_weights_h1 = np.dot(dzh1_dwh1.T, dcost_dzh1)\n",
        "    dcost_bias_h1 = dcost_dzh1\n",
        "    # Updating weights and biases\n",
        "    updated_weights = [weights[0]-lr*dcost_weights_h1,weights[1]-lr*dcost_weights_h2,weights[2]-lr*dcost_weights_o]\n",
        "    updated_bias = [bias[0]-lr*dcost_bias_h1,bias[1]-lr*dcost_bias_h2,bias[2]-lr*dcost_bias_o]\n",
        "    return updated_weights,updated_bias\n",
        "\n",
        "weights, bias = initialize_double_layer_act(X_train,hidden_layer_neurons = 10)\n",
        "for i in range(500):\n",
        "    print ('Epoch #',i)\n",
        "    try:\n",
        "      weights,bias = update_weights_double_layer_act(X_train,Y_train,weights,bias,lr = 0.001,activation = 'sigmoid')\n",
        "    except ParameterError as e:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y3E2Lty1JnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Problem 6 (Two hidden layers with activation function and momentum)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Softmax activation function for the whole output - carries out softmax in each row\n",
        "def softmax(x):\n",
        "    lis = []\n",
        "    for logits in x:\n",
        "        exps = [np.exp(logit-np.max(logits)) for logit in logits]\n",
        "        exp_sum = sum(exps)\n",
        "        out = [j/exp_sum for j in exps]\n",
        "        lis.append(out)\n",
        "    return np.asarray(lis)\n",
        "\n",
        "# Cross entropy function\n",
        "# The epsilon is used to clip values make the np.log(predictions) infinitely large\n",
        "def cross_entropy(predictions, targets, epsilon=1e-12):\n",
        "    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
        "    n = predictions.shape[0]\n",
        "    ce = -np.sum(targets*np.log(predictions+1e-9))/n\n",
        "    return ce\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid function\n",
        "def sigmoid_der(x):\n",
        "    return sigmoid(x) *(1-sigmoid (x))\n",
        "\n",
        "# TanH function\n",
        "def tanh(x):\n",
        "    temp = 2/(1+np.exp(-2*x))\n",
        "    return temp - 1\n",
        "\n",
        "# Derivative of TanH function\n",
        "def tanh_der(x):\n",
        "    return 1 - (tanh(x)*tanh(x))\n",
        "\n",
        "# ReLU function\n",
        "def relu(x):\n",
        "    return x * (x > 0) \n",
        "\n",
        "# Derivative of ReLU function\n",
        "def relu_der(x):\n",
        "    return 1. * (x > 0)\n",
        "\n",
        "# References to functions and derivatives\n",
        "f1 = sigmoid\n",
        "f2 = tanh\n",
        "f3 = relu\n",
        "f = [f1, f2, f3]\n",
        "d1 = sigmoid_der\n",
        "d2 = tanh_der\n",
        "d3 = relu_der\n",
        "d = [d1, d2, d3]\n",
        "\n",
        "# Exception class for any error in the parameters provided\n",
        "class ParameterError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "# Initialization\n",
        "def initialize_double_layer_act(X, hidden_layer_neurons):\n",
        "    input_neurons = X.shape[1]\n",
        "    output_neurons = 10\n",
        "    weights_h1 = np.random.randn(input_neurons,hidden_layer_neurons)\n",
        "    bias_h1 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_h2 = np.random.randn(hidden_layer_neurons,hidden_layer_neurons)\n",
        "    bias_h2 = np.random.randn(hidden_layer_neurons)\n",
        "    weights_o = np.random.randn(hidden_layer_neurons,output_neurons)\n",
        "    bias_o = np.random.randn(output_neurons)\n",
        "    weights = [weights_h1, weights_h2,weights_o]\n",
        "    bias = [bias_h1, bias_h2, bias_o]\n",
        "    return weights, bias\n",
        "\n",
        "# Main implementation\n",
        "def update_weights_double_layer_act(X,Y,weights,bias,lr,activation,momentum):\n",
        "    global prev_dcost_weights_o, prev_dcost_bias_o, prev_dcost_weights_h2, prev_dcost_bias_h2, prev_dcost_weights_h1, prev_dcost_bias_h1\n",
        "    # Changing activation function according to the parameter\n",
        "    if activation == 'sigmoid':\n",
        "        func = f1\n",
        "        deriv = d1\n",
        "    elif activation == 'tanh':\n",
        "        func = f2\n",
        "        deriv = d2\n",
        "    elif activation == 'relu':\n",
        "        func = f3\n",
        "        deriv = d3\n",
        "    else:\n",
        "        print(\"Invalid activation function\")\n",
        "        raise ParameterError('Activation function incorrect')\n",
        "    ###### Feedforward\n",
        "    ### For h1 layer (hidden)\n",
        "    zh1 = np.dot(X,weights[0]) + bias[0]\n",
        "    ah1 = func(zh1)\n",
        "    ### For h2 layer(hidden)\n",
        "    zh2 = np.dot(ah1,weights[1]) + bias[1]\n",
        "    ah2 = func(zh2)\n",
        "    ### For output layer\n",
        "    zo = np.dot(ah2,weights[2]) + bias[2]\n",
        "    ao = softmax(zo)\n",
        "    ###### Backpropagation\n",
        "    target = np.asarray(pd.get_dummies(Y))\n",
        "    predictions = ao\n",
        "    # Printing loss value\n",
        "    print(\"Loss: \", cross_entropy(predictions,target))\n",
        "    ### For the output layer\n",
        "    dcost_dzo = predictions - target\n",
        "    dzo_dwo = ah2\n",
        "    # Updating weights in accordance with previous weights using momentum\n",
        "    dcost_weights_o = momentum*prev_dcost_weights_o + (1-momentum)*np.dot(dzo_dwo.T,dcost_dzo)\n",
        "    dcost_bias_o = momentum*prev_dcost_bias_o + (1-momentum)*dcost_dzo\n",
        "    # Changing previous weights to current weights\n",
        "    prev_dcost_weights_o = dcost_weights_o\n",
        "    prev_dcost_bias_o = dcost_bias_o\n",
        "    ### For h2 layer (hidden)\n",
        "    dzo_dah2 = weights[2]\n",
        "    dcost_dah2 = np.dot(dcost_dzo , dzo_dah2.T)\n",
        "    dah2_dzh2 = deriv(zh2)\n",
        "    dcost_dzh2 = dah2_dzh2 * dcost_dah2\n",
        "    dzh2_dwh2 = ah1\n",
        "    # Updating weights in accordance with previous weights using momentum\n",
        "    dcost_weights_h2 = momentum*prev_dcost_weights_h2 + (1-momentum)*np.dot(dzh2_dwh2.T, dcost_dzh2)\n",
        "    dcost_bias_h2 = momentum*prev_dcost_bias_h2 + (1-momentum)*dcost_dzh2\n",
        "    # Changing previous weights to current weights\n",
        "    prev_dcost_weights_h2 = dcost_weights_h2\n",
        "    prev_dcost_bias_h2 = dcost_bias_h2\n",
        "    ### For h1 layer (hidden)\n",
        "    dzh2_dah1 = weights[1]\n",
        "    dcost_dah1 = np.dot(dcost_dzh2, dzh2_dah1.T)\n",
        "    dah1_dzh1 = deriv(zh1)\n",
        "    dcost_dzh1 = dah1_dzh1 * dcost_dah1\n",
        "    dzh1_dwh1 = X\n",
        "    # Updating weights in accordance with previous weights using momentum\n",
        "    # Arpit Omprakash\n",
        "    dcost_weights_h1 = momentum*prev_dcost_weights_h1 + (1-momentum)*np.dot(dzh1_dwh1.T, dcost_dzh1)\n",
        "    dcost_bias_h1 = momentum*prev_dcost_bias_h1 + (1-momentum)*dcost_dzh1\n",
        "    # Changing previous weights to current weights\n",
        "    prev_dcost_weights_h1 = dcost_weights_h1\n",
        "    prev_dcost_bias_h1 = dcost_bias_h1\n",
        "    ###### Updating weights and biases\n",
        "    updated_weights = [weights[0]-lr*dcost_weights_h1,weights[1]-lr*dcost_weights_h2,weights[2]-lr*dcost_weights_o]\n",
        "    updated_bias = [bias[0]-lr*dcost_bias_h1,bias[1]-lr*dcost_bias_h2,bias[2]-lr*dcost_bias_o]\n",
        "    return updated_weights,updated_bias\n",
        "\n",
        "# Function that can takes epoch value and trains the neural network\n",
        "def training_nn(X,Y,weights,bias,lr,activation,momentum,epochs = 10):\n",
        "    for epoch in range(epochs):\n",
        "        print ('Epoch #', epoch)\n",
        "        try:\n",
        "          weights,bias = update_weights_double_layer_act(X,Y,weights,bias,lr,activation,momentum)\n",
        "        except ParameterError:\n",
        "          break\n",
        "    return weights\n",
        "\n",
        "# Variables to store previous gradients (for implementing momentum)\n",
        "prev_dcost_weights_o = 0\n",
        "prev_dcost_bias_o = 0\n",
        "prev_dcost_weights_h2 = 0\n",
        "prev_dcost_bias_h2 = 0\n",
        "prev_dcost_weights_h1 = 0\n",
        "prev_dcost_bias_h1 = 0\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Initializing weights and bias\n",
        "weights, bias = initialize_double_layer_act(X_train,hidden_layer_neurons = 10)\n",
        "# Training the model for \"epoch\" number of epochs\n",
        "# The training returns the resulting weights after training\n",
        "training_nn(X_train, Y_train, weights, bias, lr = 0.01, activation = 'sigmoid', momentum = 0.9, epochs = 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}